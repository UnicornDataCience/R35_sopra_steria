import streamlit as st
import pandas as pd
import numpy as np
import asyncio
import os
import sys
import re
import json
import inspect
import concurrent.futures
import traceback
from datetime import datetime
from dotenv import load_dotenv

# A√±adir la ruta del proyecto al path de Python
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# Cargar variables de entorno
load_dotenv()

# FORZAR GROQ - Sobrescribir variable del sistema si FORCE_GROQ=true
if os.getenv('FORCE_GROQ', 'false').lower() == 'true':
    os.environ['LLM_PROVIDER'] = 'groq'
    print("üöÄ [STREAMLIT] Variable LLM_PROVIDER forzada a 'groq'")

# Importar el wrapper s√≠ncrono para evitar problemas de event loop
try:
    from src.utils.streamlit_async_wrapper import run_async_safe
    ASYNC_WRAPPER_AVAILABLE = True
    print("‚úÖ Wrapper s√≠ncrono disponible")
except ImportError as e:
    ASYNC_WRAPPER_AVAILABLE = False
    print(f"‚ö†Ô∏è Wrapper s√≠ncrono no disponible: {e}")

# Importar el selector de columnas m√©dicas
try:
    from src.adapters.medical_column_selector import MedicalColumnSelector
    COLUMN_SELECTOR_AVAILABLE = True
except ImportError as e:
    COLUMN_SELECTOR_AVAILABLE = False
    print(f"‚ö†Ô∏è Selector de columnas no disponible: {e}")

# Configuraci√≥n de p√°gina
st.set_page_config(
    page_title="Patient IA",
    page_icon=os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "assets", "logo_patientia.png"),
    layout="wide",
    initial_sidebar_state="collapsed"
)

# CSS personalizado para un dise√±o moderno
st.markdown("""
<style>
    /* Ocultar elementos innecesarios */
    .stDeployButton {display:none;}
    footer {visibility: hidden;}
    .stMainBlockContainer {padding-top: 2rem;}
    
    /* Estilo del chat */
    .chat-container {
        max-width: 800px;
        margin: 0 auto;
        padding: 0 1rem;
    }
    
    /* Prompt input m√°s grande */
    .stChatInput {
        position: relative;
        max-width: 700px !important;
        margin: 0 auto !important;
    }
    
    .stChatInput > div > div > textarea {
        min-height: 60px !important;
        font-size: 16px !important;
        border-radius: 25px !important;
        border: 2px solid #e1e5e9 !important;
        padding: 15px 75px 15px 60px !important;
        text-indent: 10ch !important;
    }
    
    /* Header minimalista */
    .main-header {
        text-align: center;
        margin-bottom: 0.5rem;
        padding: 1rem 0 0 0;
    }
    
    .main-header h1 {
        font-size: 2.5rem;
        font-weight: 600;
        color: #1f2937;
        margin-bottom: 0.5rem;
    }
    
    /* Logo centrado */
    .logo-container {
        text-align: center;
        margin-bottom: 0.5rem;
    }
    
    .logo-container img {
        margin: 0 auto;
        display: block;
    }
    
    /* Status indicator centrado */
    .status-indicator {
        display: flex;
        justify-content: center;
        align-items: center;
        gap: 8px;
        background: #f0f9ff;
        border: 1px solid #bfdbfe;
        border-radius: 20px;
        padding: 6px 12px;
        font-size: 0.875rem;
        color: #1e40af;
        margin: 0.5rem auto 1rem auto;
        width: fit-content;
    }
    
    /* Mensaje de bienvenida centrado */
    .welcome-container {
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        min-height: 40vh;
        margin-top: 0;
    }
    
    /* Mensajes del chat */
    .stChatMessage {
        margin-bottom: 1rem;
    }
</style>
""", unsafe_allow_html=True)

# Importar configuraci√≥n unificada de LLMs
try:
    from src.config.llm_config import unified_llm_config
    LLM_CONFIGURED = True
    connection_test = unified_llm_config.test_connection()
    provider_info = unified_llm_config.status_info
    active_provider = provider_info["active_provider"]
    
    if connection_test:
        print(f"‚úÖ LLM conectado correctamente - Proveedor: {active_provider}")
    else:
        print(f"‚ö†Ô∏è LLM configurado pero sin conexi√≥n - Proveedor: {active_provider}")
        
except Exception as e:
    LLM_CONFIGURED = False
    connection_test = False
    active_provider = "none"
    print(f"‚ö†Ô∏è Error de configuraci√≥n LLM: {e}")

# Mantener compatibilidad con c√≥digo existente
AZURE_CONFIGURED = LLM_CONFIGURED and active_provider == "azure"

# Importar orquestador con fallback para problemas de MRO en LangGraph
try:
    from src.orchestration.langgraph_orchestrator import MedicalAgentsOrchestrator, AgentState
    LANGGRAPH_AVAILABLE = True
    print("‚úÖ LangGraph Orchestrator disponible")
except Exception as e:
    print(f"‚ÑπÔ∏è Usando Simple Orchestrator (LangGraph tiene problema de dependencias)")
    try:
        from src.orchestration.simple_orchestrator import MedicalAgentsOrchestrator
        LANGGRAPH_AVAILABLE = False
        print("‚úÖ Usando Simple Orchestrator como fallback")
    except Exception as e2:
        print(f"‚ùå Error cargando orquestador alternativo: {e2}")
        LANGGRAPH_AVAILABLE = False

# Importar el nuevo FastOrchestrator como alternativa optimizada
try:
    from src.orchestration.fast_orchestrator import FastMedicalOrchestrator
    FAST_ORCHESTRATOR_AVAILABLE = True
    print("‚úÖ Fast Orchestrator disponible")
except Exception as e:
    FAST_ORCHESTRATOR_AVAILABLE = False
    print(f"‚ö†Ô∏è Fast Orchestrator no disponible: {e}")

try:
    from src.agents.base_agent import BaseLLMAgent, BaseAgentConfig
    from src.agents.coordinator_agent import CoordinatorAgent
    from src.agents.analyzer_agent import ClinicalAnalyzerAgent
    from src.agents.generator_agent import SyntheticGeneratorAgent
    
    # Importar otros agentes con manejo individual de errores
    try:
        from src.agents.validator_agent import MedicalValidatorAgent
        VALIDATOR_AVAILABLE = True
    except Exception as e:
        print(f"‚ö†Ô∏è Validator agent no disponible: {e}")
        VALIDATOR_AVAILABLE = False
    
    try:
        from src.agents.simulator_agent import PatientSimulatorAgent
        SIMULATOR_AVAILABLE = True
    except Exception as e:
        print(f"‚ö†Ô∏è Simulator agent no disponible: {e}")
        SIMULATOR_AVAILABLE = False
    
    try:
        from src.agents.evaluator_agent import UtilityEvaluatorAgent
        EVALUATOR_AVAILABLE = True
    except Exception as e:
        print(f"‚ö†Ô∏è Evaluator agent no disponible: {e}")
        EVALUATOR_AVAILABLE = False

    AGENTS_AVAILABLE = True
    LANGGRAPH_AVAILABLE = True
    print("‚úÖ LangGraph Orchestrator y Agentes cargados correctamente")
    
except Exception as e:
    AGENTS_AVAILABLE = False
    LANGGRAPH_AVAILABLE = False
    VALIDATOR_AVAILABLE = False
    SIMULATOR_AVAILABLE = False
    EVALUATOR_AVAILABLE = False
    print(f"‚ùå Error cargando agentes: {e}")

# Agente mock para desarrollo
class MockAgent:
    def __init__(self, name):
        self.name = name
        self.config = type('Config', (), {'name': name})()
    
    async def process(self, input_text, context=None):
        return self.process_sync(input_text, context)
    
    def process_sync(self, input_text, context=None):
        context = context or {}
        has_dataset = context.get("dataset_uploaded", False)
        
        if "coordinador" in self.name.lower():
            dataset_msg = ""
            if has_dataset:
                filename = context.get("filename", "archivo")
                rows = context.get("rows", 0)
                cols = context.get("columns", 0)
                dataset_msg = f"\n\nDataset detectado: {filename} ({rows:,} filas, {cols} columnas)"
            
            return {
                "message": f"üëã **¬°Hola!** Soy tu asistente de IA para generar datos cl√≠nicos sint√©ticos.\n\nüî¨ **Estado:** {'‚úÖ ' + active_provider.title() + ' Conectado' if connection_test else 'üîÑ Modo Simulado'}{dataset_msg}\n\n**üß† Mi equipo especializado:**\n‚Ä¢ **Analista** - Extrae patrones cl√≠nicos\n‚Ä¢ **Generador** - Crea datos sint√©ticos con SDV\n‚Ä¢ **Validador** - Verifica coherencia m√©dica\n‚Ä¢ **Simulador** - Modela evoluci√≥n temporal\n‚Ä¢ **Evaluador** - Mide calidad y utilidad\n\n¬øEn qu√© puedo ayudarte hoy?",
                "agent": self.name,
                "mock": True
            }
        elif "analista" in self.name.lower():
            if has_dataset:
                filename = context.get("filename", "dataset")
                rows = context.get("rows", 0)
                cols = context.get("columns", 0)
                return {
                    "message": f"""üîç **An√°lisis completado (modo simulado)**

**üìä Dataset analizado:** {filename}
- **Registros:** {rows:,}
- **Columnas:** {cols}

**üìà An√°lisis estad√≠stico:**
- Tipos de datos identificados
- Valores faltantes detectados  
- Distribuciones analizadas
- Correlaciones calculadas

**üí° Insights principales:**
- Dataset preparado para generaci√≥n sint√©tica
- Calidad de datos: Buena
- Recomendaci√≥n: Proceder con generaci√≥n TVAE o CTGAN

*Nota: An√°lisis completo disponible con Azure OpenAI configurado.*""",
                    "agent": self.name,
                    "mock": True
                }
            else:
                return {
                    "message": "üìÅ **No hay dataset cargado**\n\nPara an√°lisis cl√≠nico, necesito que subas un archivo CSV o Excel con datos m√©dicos.\n\nüìä **Formatos aceptados:** CSV, XLSX, XLS",
                    "agent": self.name,
                    "mock": True
                }
        elif "generador" in self.name.lower():
            if has_dataset:
                # Obtener par√°metros de generaci√≥n
                params = context.get("parameters", {})
                model_type = params.get("model_type", "ctgan")
                num_samples = params.get("num_samples", 100)
                
                # Crear datos sint√©ticos simulados
                original_df = context.get("dataframe")
                if original_df is not None:
                    # Crear una muestra sint√©tica usando el DataFrame original
                    synthetic_data = original_df.sample(n=min(num_samples, len(original_df)), replace=True).reset_index(drop=True)
                    
                    # A√±adir algo de ruido para simular diferencias
                    import numpy as np
                    for col in synthetic_data.select_dtypes(include=[np.number]).columns:
                        noise = np.random.normal(0, synthetic_data[col].std() * 0.05, len(synthetic_data))
                        synthetic_data[col] = synthetic_data[col] + noise
                    
                    return {
                        "message": f"""üß¨ **Generaci√≥n sint√©tica completada**

**üìä Resultado:**
- **Modelo utilizado:** {model_type.upper()}
- **Registros generados:** {len(synthetic_data):,}
- **Dataset base:** {len(original_df):,} registros

**‚úÖ Calidad de datos:** Los datos sint√©ticos mantienen las propiedades estad√≠sticas del dataset original mientras preservan la privacidad.""",
                        "agent": self.name,
                        "synthetic_data": synthetic_data,
                        "generation_info": {
                            "model_type": model_type,
                            "num_samples": len(synthetic_data),
                            "columns_used": len(synthetic_data.columns),
                            "selection_method": "Mock Generation",
                            "timestamp": pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')
                        }
                    }
                else:
                    rows = context.get("rows", 0)
                    return {
                        "message": f"""üß¨ **Generaci√≥n sint√©tica completada (modo simulado)**

**üìä Dataset base:** {rows:,} registros
**üéØ Registros generados:** {num_samples} (simulado)
**üî¨ Modelo utilizado:** {model_type.upper()} (simulado)

**‚úÖ Proceso completado:**
- Datos sint√©ticos generados exitosamente
- Calidad preservada
- Privacidad garantizada

*Nota: Generaci√≥n real disponible con Azure OpenAI configurado.*""",
                        "agent": self.name,
                        "mock": True
                    }
            else:
                return {
                    "message": "üìÅ **Dataset requerido**\n\nPara generar datos sint√©ticos, primero necesito un dataset base.\n\n**Sube un archivo** con datos cl√≠nicos para comenzar.",
                    "agent": self.name,
                    "mock": True
                }
        else:
            return {
                "message": f"ü§ñ **{self.name}**\n\n{input_text}\n\n*Funcionalidad completa disponible con Azure OpenAI configurado.*",
                "agent": self.name,
                "mock": True
            }

def clean_response_message(message: str) -> str:
    """
    Limpia el mensaje de respuesta de caracteres Unicode escapados, JSON crudo y problemas de encoding.
    Dise√±ado espec√≠ficamente para el problema de doble encoding UTF-8/Latin-1 del modelo LLM.
    """
    
    # M√âTODO 0: Detectar y extraer JSON crudo m√°s robustamente
    message_stripped = message.strip()
    
    # Casos de JSON directo
    if message_stripped.startswith('{"intention"') or message_stripped.startswith('{"message"'):
        try:
            parsed = json.loads(message_stripped)
            if 'message' in parsed:
                message = parsed['message']
                print("‚úÖ [CLEAN] JSON crudo extra√≠do exitosamente")
            elif 'content' in parsed:
                message = parsed['content']
                print("‚úÖ [CLEAN] Contenido JSON extra√≠do")
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è [CLEAN] Error parseando JSON crudo: {e}")
            # Si no se puede parsear, mantener el mensaje original
            pass
    
    # Casos de JSON anidado o mal formateado
    elif '"message"' in message_stripped and '"intention"' in message_stripped:
        try:
            # Buscar el patr√≥n JSON dentro del texto
            import re
            json_pattern = r'\{"intention".*?"message"[^}]*\}'
            match = re.search(json_pattern, message_stripped, re.DOTALL)
            if match:
                parsed = json.loads(match.group())
                if 'message' in parsed:
                    message = parsed['message']
                    print("‚úÖ [CLEAN] JSON anidado extra√≠do")
        except:
            pass
    
    try:
        # M√âTODO 1: Detecci√≥n y correcci√≥n m√°s agresiva del doble encoding UTF-8/Latin-1
        # Este es el problema principal con el modelo llama-4-scout
        
        # Lista de indicadores de corrupci√≥n m√°s completa
        corrupted_patterns = [
            '√Ç¬°', '√Ç¬ø', '√É¬°', '√É¬©', '√É¬≠', '√É¬≥', '√É¬∫', '√É¬±', '√É‚Ä∞', '√É"', '√É≈°', '√É√ë',
            'm√É¬©dica', 'm√É¬©dico', 'an√É¬°lisis', 's√É¬≠ntomas', 'diagn√É¬≥stico', 'informaci√É¬≥n',
            'especializaci√É¬≥n', 'atenci√É¬≥n', 'evaluaci√É¬≥n', 'investigaci√É¬≥n', 'aqu√É¬≠',
            'tambi√É¬©n', 'pr√É¬°ctica', 'cl√É¬≠nicos', 'p√É¬°gina', 'd√É¬≠a', 'f√É¬°cil', 'dif√É¬≠cil',
            'qu√É¬©', 'c√É¬≥mo', 'd√É¬≥nde', 'cu√É¬°ndo'
        ]
        
        if any(pattern in message for pattern in corrupted_patterns):
            print("üîç [CLEAN] Detectada corrupci√≥n de encoding, aplicando correcci√≥n")
            
            # ESTRATEGIA M√öLTIPLE DE CORRECCI√ìN
            original_message = message
            attempts = []
            
            # Intento 1: Correcci√≥n autom√°tica est√°ndar
            try:
                corrected1 = message.encode('latin-1').decode('utf-8')
                attempts.append(('auto_standard', corrected1))
            except:
                pass
            
            # Intento 2: Correcci√≥n con manejo de errores 'ignore'
            try:
                corrected2 = message.encode('latin-1', errors='ignore').decode('utf-8', errors='ignore')
                attempts.append(('auto_ignore', corrected2))
            except:
                pass
            
            # Intento 3: Correcci√≥n manual con diccionario m√°s completo
            corrected3 = message
            corruption_map = {
                # Signos de puntuaci√≥n
                '√Ç¬°': '¬°', '√Ç¬ø': '¬ø',
                # Vocales con tilde min√∫sculas
                '√É¬°': '√°', '√É¬©': '√©', '√É¬≠': '√≠', '√É¬≥': '√≥', '√É¬∫': '√∫',
                # E√±e
                '√É¬±': '√±', '√É√ë': '√ë',
                # Palabras completas muy comunes (m√°s directas)
                'm√É¬©dica': 'm√©dica', 'm√É¬©dico': 'm√©dico', 'm√É¬©dicos': 'm√©dicos',
                'an√É¬°lisis': 'an√°lisis', 's√É¬≠ntomas': 's√≠ntomas', 's√É¬≠ntoma': 's√≠ntoma',
                'diagn√É¬≥stico': 'diagn√≥stico', 'diagn√É¬≥sticos': 'diagn√≥sticos',
                'informaci√É¬≥n': 'informaci√≥n', 'atenci√É¬≥n': 'atenci√≥n',
                'especializaci√É¬≥n': 'especializaci√≥n', 'evaluaci√É¬≥n': 'evaluaci√≥n',
                'investigaci√É¬≥n': 'investigaci√≥n', 'pr√É¬°ctica': 'pr√°ctica',
                'cl√É¬≠nicos': 'cl√≠nicos', 'cl√É¬≠nica': 'cl√≠nica', 'cl√É¬≠nico': 'cl√≠nico',
                'p√É¬°gina': 'p√°gina', 'p√É¬°ginas': 'p√°ginas',
                'd√É¬≠a': 'd√≠a', 'd√É¬≠as': 'd√≠as',
                'aqu√É¬≠': 'aqu√≠', 'all√É¬≠': 'all√≠', 'tambi√É¬©n': 'tambi√©n',
                'f√É¬°cil': 'f√°cil', 'dif√É¬≠cil': 'dif√≠cil',
                # Interrogativos
                'qu√É¬©': 'qu√©', 'c√É¬≥mo': 'c√≥mo', 'd√É¬≥nde': 'd√≥nde', 'cu√É¬°ndo': 'cu√°ndo',
                'qui√É¬©n': 'qui√©n', 'cu√É¬°l': 'cu√°l', 'cu√É¬°nto': 'cu√°nto'
            }
            
            for corrupt, correct in corruption_map.items():
                if corrupt in corrected3:
                    corrected3 = corrected3.replace(corrupt, correct)
            
            attempts.append(('manual_dict', corrected3))
            
            # Evaluar cu√°l correcci√≥n es mejor
            def count_spanish_chars(text):
                spanish_chars = ['√°', '√©', '√≠', '√≥', '√∫', '√±', '¬°', '¬ø', '√Å', '√â', '√ç', '√ì', '√ö', '√ë']
                return sum(1 for char in text if char in spanish_chars)
            
            def count_corrupted_chars(text):
                corrupted = ['√É', '√Ç']
                return sum(1 for char in text if char in corrupted)
            
            # Seleccionar la mejor correcci√≥n
            best_attempt = None
            best_score = -1
            
            for method, corrected in attempts:
                spanish_count = count_spanish_chars(corrected)
                corrupted_count = count_corrupted_chars(corrected)
                score = spanish_count - (corrupted_count * 2)  # Penalizar caracteres corruptos
                
                if score > best_score:
                    best_score = score
                    best_attempt = (method, corrected)
            
            if best_attempt:
                message = best_attempt[1]
                print(f"‚úÖ [CLEAN] Mejor correcci√≥n: {best_attempt[0]} (score: {best_score})")
        
        # M√âTODO 2: Limpiar secuencias Unicode escapadas (ya funciona bien)
        unicode_patterns = [
            (r'\\u00a1', '¬°'), (r'\\u00bf', '¬ø'),
            (r'\\u00e1', '√°'), (r'\\u00e9', '√©'), (r'\\u00ed', '√≠'), 
            (r'\\u00f3', '√≥'), (r'\\u00fa', '√∫'), (r'\\u00f1', '√±'),
            (r'\\u00c1', '√Å'), (r'\\u00c9', '√â'), (r'\\u00cd', '√ç'), 
            (r'\\u00d3', '√ì'), (r'\\u00da', '√ö'), (r'\\u00d1', '√ë')
        ]
        
        unicode_replaced = False
        for pattern, replacement in unicode_patterns:
            if re.search(pattern, message, flags=re.IGNORECASE):
                message = re.sub(pattern, replacement, message, flags=re.IGNORECASE)
                unicode_replaced = True
        
        if unicode_replaced:
            print("‚úÖ [CLEAN] Secuencias Unicode escapadas convertidas")
        
        # M√âTODO 3: Limpieza final conservadora
        try:
            decoded = message.encode().decode('unicode_escape')
            if decoded != message and not any(char in decoded for char in ['√É', '√Ç']):
                message = decoded
                print("‚úÖ [CLEAN] Unicode escape final aplicado")
        except:
            pass
            
    except Exception as e:
        print(f"‚ö†Ô∏è [CLEAN] Error en limpieza de mensaje: {e}")
    
    return message.strip()

@st.cache_resource
def initialize_orchestrator():
    """
    Inicializa el orquestador principal con prioridad para FastOrchestrator.
    FastOrchestrator es optimizado para respuestas r√°pidas sin timeout.
    """
    # PRIORIDAD 1: FastOrchestrator (Optimizado para velocidad)
    if FAST_ORCHESTRATOR_AVAILABLE and LLM_CONFIGURED:
        try:
            agents = {}
            
            # Cargar agentes disponibles para workflows espec√≠ficos
            if AGENTS_AVAILABLE:
                try:
                    agents["coordinator"] = CoordinatorAgent()
                    agents["analyzer"] = ClinicalAnalyzerAgent()
                    agents["generator"] = SyntheticGeneratorAgent()
                    
                    if VALIDATOR_AVAILABLE:
                        agents["validator"] = MedicalValidatorAgent()
                        print("‚úÖ Validator agent agregado a FastOrchestrator")
                    
                    if SIMULATOR_AVAILABLE:
                        agents["simulator"] = PatientSimulatorAgent()
                        print("‚úÖ Simulator agent agregado a FastOrchestrator")
                    
                    if EVALUATOR_AVAILABLE:
                        agents["evaluator"] = UtilityEvaluatorAgent()
                        print("‚úÖ Evaluator agent agregado a FastOrchestrator")
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è Algunos agentes no disponibles para FastOrchestrator: {e}")
            
            fast_orchestrator = FastMedicalOrchestrator(agents)
            print("üöÄ FastMedicalOrchestrator inicializado (modo optimizado)")
            return fast_orchestrator
            
        except Exception as e:
            print(f"‚ùå Error inicializando FastOrchestrator: {e}")
    
    # PRIORIDAD 2: LangGraph Orchestrator (Original)
    if AGENTS_AVAILABLE and LANGGRAPH_AVAILABLE and LLM_CONFIGURED:
        try:
            agents = {
                "coordinator": CoordinatorAgent(),
                "analyzer": ClinicalAnalyzerAgent(),
                "generator": SyntheticGeneratorAgent(),
            }
            
            # Agregar agentes opcionales solo si est√°n disponibles
            if VALIDATOR_AVAILABLE:
                agents["validator"] = MedicalValidatorAgent()
                print("‚úÖ Validator agent agregado")
            
            if SIMULATOR_AVAILABLE:
                agents["simulator"] = PatientSimulatorAgent()
                print("‚úÖ Simulator agent agregado")
            
            if EVALUATOR_AVAILABLE:
                agents["evaluator"] = UtilityEvaluatorAgent()
                print("‚úÖ Evaluator agent agregado")
            
            orchestrator = MedicalAgentsOrchestrator(agents)
            print("‚úÖ LangGraph Orchestrator inicializado con agentes reales")
            return orchestrator
        except Exception as e:
            print(f"‚ùå Error en LangGraph: {e}")
    
    # PRIORIDAD 3: Mock Orchestrator (Fallback)
    print("‚ö†Ô∏è Usando orquestador mock")
    return create_mock_orchestrator()

@st.cache_resource 
def initialize_langgraph_orchestrator():
    """Funci√≥n legacy para compatibilidad - ahora redirige a initialize_orchestrator"""
    return initialize_orchestrator()

def create_mock_orchestrator():
    """Crea un orquestador mock para desarrollo"""
    mock_agents = {
        "coordinator": MockAgent("Coordinador"),
        "analyzer": MockAgent("Analista Cl√≠nico"),
        "generator": MockAgent("Generador Sint√©tico"),
        "validator": MockAgent("Validador M√©dico"),
        "simulator": MockAgent("Simulador de Pacientes"),
        "evaluator": MockAgent("Evaluador de Utilidad")
    }
    
    class MockLangGraphOrchestrator:
        def __init__(self, agents):
            self.agents = agents
            self.state = {
                "current_agent": "coordinator",
                "dataset_uploaded": False,
                "analysis_complete": False,
                "generation_complete": False,
                "validation_complete": False,
                "synthetic_data": None,
                "context": {}
            }
        
        async def process_user_input(self, user_input: str, context: dict = None):
            """Procesa input del usuario (versi√≥n mock)"""
            return self.process_user_input_sync(user_input, context)
        
        def process_user_input_sync(self, user_input: str, context: dict = None):
            """Versi√≥n s√≠ncrona del procesamiento de input del usuario"""
            self.state["context"] = context or {}
            
            # Detectar intenci√≥n y ejecutar agente correspondiente
            if any(word in user_input.lower() for word in ["analizar", "an√°lisis", "analiza"]):
                self.state["current_agent"] = "analyzer"
                agent = self.agents["analyzer"]
                response = agent.process_sync(user_input, context)
                return response
            elif any(word in user_input.lower() for word in ["generar", "sint√©tico", "sint√©ticos", "genera"]):
                self.state["current_agent"] = "generator"
                
                # Detectar modelo espec√≠fico en lenguaje natural
                model_type = "ctgan"  # Default
                if any(word in user_input.lower() for word in ["tvae", "variational", "autoencoder"]):
                    model_type = "tvae"
                elif any(word in user_input.lower() for word in ["sdv", "vault", "synthetic data vault"]):
                    model_type = "sdv"
                elif any(word in user_input.lower() for word in ["ctgan", "gan", "generative adversarial"]):
                    model_type = "ctgan"
                
                # Detectar n√∫mero de muestras
                import re
                numbers = re.findall(r'\b(\d+)\b', user_input)
                num_samples = int(numbers[0]) if numbers else 100
                
                # Configurar contexto con par√°metros
                context = context or {}
                context['parameters'] = {
                    'model_type': model_type,
                    'num_samples': num_samples
                }
                
                agent = self.agents["generator"]
                response = agent.process_sync(user_input, context)
                return response
            elif any(word in user_input.lower() for word in ["validar", "valida", "validaci√≥n"]):
                self.state["current_agent"] = "validator"
                # Usar el validador real si est√° disponible, incluso en modo mock
                if VALIDATOR_AVAILABLE:
                    try:
                        agent = self.agents["validator"]
                        # Para el agente validador real que puede ser async, usar el wrapper
                        if hasattr(agent, 'process_sync'):
                            response = agent.process_sync(user_input, context)
                        else:
                            # Si es async, usar run_async_safe si est√° disponible
                            if ASYNC_WRAPPER_AVAILABLE:
                                response = run_async_safe(agent.process, user_input, context)
                            else:
                                response = {"message": "‚ùå Error: Agente async sin wrapper disponible", "agent": "validator", "error": True}
                        return response
                    except Exception as e:
                        return {"message": f"‚ùå Error en validaci√≥n: {str(e)}", "agent": "validator", "error": True}
                else:
                    return {"message": "‚úÖ Validaci√≥n completada (modo simulado)\n\nDatos validados exitosamente.", "agent": "validator"}
            elif any(word in user_input.lower() for word in ["evaluar", "eval√∫a", "calidad"]):
                self.state["current_agent"] = "evaluator"
                return {"message": "üìä Evaluaci√≥n completada (modo simulado)\n\nCalidad de datos: Excelente", "agent": "evaluator"}
            elif any(word in user_input.lower() for word in ["simular", "simula", "paciente"]):
                self.state["current_agent"] = "simulator"
                return {"message": "üè• Simulaci√≥n completada (modo simulado)\n\nEvoluci√≥n de pacientes simulada.", "agent": "simulator"}
            else:
                # Para preguntas conversacionales, m√©dicas o generales
                self.state["current_agent"] = "coordinator"
                
                # Si Azure est√° configurado, usar el coordinador real
                if AZURE_CONFIGURED and connection_test:
                    agent = self.agents["coordinator"]
                    # Usar m√©todo s√≠ncrono o wrapper seg√∫n sea necesario
                    if hasattr(agent, 'process_sync'):
                        response = agent.process_sync(user_input, context)
                    elif ASYNC_WRAPPER_AVAILABLE:
                        response = run_async_safe(agent.process, user_input, context)
                    else:
                        response = {"message": "‚ùå Error: Agente async sin wrapper disponible", "agent": "coordinator", "error": True}
                    return response
                else:
                    # Respuesta mock inteligente para preguntas m√©dicas
                    return self._generate_mock_medical_response(user_input, context)
        
        def _generate_mock_medical_response(self, user_input: str, context: dict = None):
            """Genera respuestas mock inteligentes para preguntas m√©dicas"""
            user_lower = user_input.lower()
            
            # Detectar diferentes tipos de preguntas m√©dicas
            if any(term in user_lower for term in ["diabetes", "glucosa", "insulina"]):
                return {
                    "message": "ü©∫ **Informaci√≥n sobre Diabetes**\n\nLa diabetes es una enfermedad cr√≥nica que afecta la forma en que el cuerpo procesa la glucosa. Los principales factores de riesgo incluyen:\n\n‚Ä¢ **Tipo 1**: Factores gen√©ticos e inmunol√≥gicos\n‚Ä¢ **Tipo 2**: Sobrepeso, sedentarismo, historia familiar\n‚Ä¢ **Gestacional**: Cambios hormonales durante el embarazo\n\n**Recomendaci√≥n**: Para an√°lisis detallado de datos diab√©ticos, sube un dataset y solicita un an√°lisis espec√≠fico.\n\n*Nota: Esta es informaci√≥n general. Consulta siempre con un profesional m√©dico.*",
                    "agent": "coordinator",
                    "topic": "diabetes"
                }
            elif any(term in user_lower for term in ["covid", "coronavirus", "sars-cov"]):
                return {
                    "message": "ü¶† **Informaci√≥n sobre COVID-19**\n\nFactores de riesgo identificados en datos cl√≠nicos:\n\n‚Ä¢ **Edad**: Pacientes > 65 a√±os\n‚Ä¢ **Comorbilidades**: Diabetes, hipertensi√≥n, EPOC\n‚Ä¢ **Estado inmunol√≥gico**: Inmunosupresi√≥n\n‚Ä¢ **Factores cardiovasculares**: Enfermedad card√≠aca previa\n\n**Nuestro sistema** puede analizar datasets COVID-19 y generar datos sint√©ticos que preserven estos patrones epidemiol√≥gicos.\n\n*Datos basados en estudios internacionales publicados.*",
                    "agent": "coordinator",
                    "topic": "covid19"
                }
            elif any(term in user_lower for term in ["hipertensi√≥n", "presi√≥n", "cardiovascular"]):
                return {
                    "message": "‚ù§Ô∏è **Factores de Riesgo Cardiovascular**\n\nPrincipales factores identificados en estudios cl√≠nicos:\n\n‚Ä¢ **Modificables**: Tabaquismo, colesterol alto, sedentarismo\n‚Ä¢ **No modificables**: Edad, sexo, historia familiar\n‚Ä¢ **Metab√≥licos**: Diabetes, obesidad, s√≠ndrome metab√≥lico\n‚Ä¢ **Otros**: Estr√©s, apnea del sue√±o, enfermedad renal\n\n**¬øTienes datos cardiovasculares?** Puedo ayudarte a analizarlos y generar datasets sint√©ticos para investigaci√≥n.\n\n*Informaci√≥n basada en gu√≠as cl√≠nicas internacionales.*",
                    "agent": "coordinator",
                    "topic": "cardiovascular"
                }
            elif any(term in user_lower for term in ["c√°ncer", "oncolog√≠a", "tumor", "met√°stasis"]):
                return {
                    "message": "üéóÔ∏è **Informaci√≥n Oncol√≥gica**\n\nFactores relevantes en an√°lisis de datos oncol√≥gicos:\n\n‚Ä¢ **Estadificaci√≥n**: TNM, grado histol√≥gico\n‚Ä¢ **Biomarcadores**: Receptores hormonales, HER2, mutaciones\n‚Ä¢ **Tratamiento**: Quimioterapia, radioterapia, inmunoterapia\n‚Ä¢ **Seguimiento**: Supervivencia libre de enfermedad, calidad de vida\n\n**Capacidades del sistema**: An√°lisis de cohortes oncol√≥gicas y generaci√≥n de datos sint√©ticos preservando caracter√≠sticas pron√≥sticas.\n\n*Para an√°lisis espec√≠ficos, considera subir datos anonimizados.*",
                    "agent": "coordinator",
                    "topic": "oncology"
                }
            elif any(term in user_lower for term in ["hola", "saludo", "buenos d√≠as", "buenas tardes", "como estas"]):
                has_dataset = context and context.get("dataset_uploaded", False)
                dataset_msg = ""
                if has_dataset:
                    filename = context.get("filename", "archivo")
                    rows = context.get("rows", 0)
                    cols = context.get("columns", 0)
                    dataset_msg = f"\n\nüìä **Dataset actual**: {filename} ({rows:,} filas, {cols} columnas)"
                
                return {
                    "message": f"üëã **¬°Hola!** Estoy muy bien, gracias por preguntar.\n\nSoy tu asistente de IA especializado en datos cl√≠nicos sint√©ticos.\n\nüî¨ **Estado del sistema**: {'‚úÖ ' + active_provider.title() + ' Conectado' if connection_test else 'üîÑ Modo Simulado'}{dataset_msg}\n\n**¬øEn qu√© puedo ayudarte?**\n‚Ä¢ Analizar datasets m√©dicos\n‚Ä¢ Generar datos sint√©ticos seguros\n‚Ä¢ Responder preguntas sobre medicina\n‚Ä¢ Validar coherencia cl√≠nica\n\n¬°Preg√∫ntame cualquier cosa sobre medicina o datos cl√≠nicos!",
                    "agent": "coordinator",
                    "topic": "greeting"
                }
            elif any(term in user_lower for term in ["ayuda", "help", "qu√© puedes hacer"]):
                return {
                    "message": "üìã **Gu√≠a de Uso - Patient IA**\n\n**ü§ñ Comandos principales:**\n‚Ä¢ `Analiza estos datos` - Explora patrones en tu dataset\n‚Ä¢ `Genera 1000 muestras con CTGAN` - Crea datos sint√©ticos\n‚Ä¢ `Valida la coherencia m√©dica` - Verifica calidad cl√≠nica\n\n**ü©∫ Consultas m√©dicas:**\n‚Ä¢ Factores de riesgo cardiovascular\n‚Ä¢ Informaci√≥n sobre diabetes, COVID-19\n‚Ä¢ An√°lisis epidemiol√≥gico\n‚Ä¢ Interpretaci√≥n de biomarcadores\n\n**üìä Tipos de datos soportados:**\n‚Ä¢ CSV, Excel (.xlsx, .xls)\n‚Ä¢ Historiales cl√≠nicos\n‚Ä¢ Datos de laboratorio\n‚Ä¢ Registros epidemiol√≥gicos\n\n¬øHay algo espec√≠fico en lo que te pueda ayudar?",
                    "agent": "coordinator",
                    "topic": "help"
                }
            else:
                # Respuesta general para otras preguntas m√©dicas
                return {
                    "message": f"ü§î **Respuesta m√©dica (modo simulado)**\n\nHe recibido tu consulta: *\"{user_input}\"*\n\nüìö Como asistente de IA m√©dica, puedo ayudarte con:\n‚Ä¢ An√°lisis de datasets cl√≠nicos\n‚Ä¢ Informaci√≥n sobre enfermedades comunes\n‚Ä¢ Interpretaci√≥n de factores de riesgo\n‚Ä¢ Generaci√≥n de datos sint√©ticos\n\n**Para respuestas m√°s precisas**, configura Azure OpenAI o formula tu pregunta de manera m√°s espec√≠fica.\n\n*Recuerda: Esta informaci√≥n es para fines educativos. Consulta siempre con profesionales m√©dicos.*",
                    "agent": "coordinator",
                    "topic": "general_medical"
                }
    
    return MockLangGraphOrchestrator(mock_agents)

# Funci√≥n initialize_orchestrator movida arriba para evitar duplicaci√≥n

# Inicializaci√≥n del estado
if 'orchestrator' not in st.session_state:
    st.session_state.orchestrator = initialize_orchestrator()

if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []

if 'context' not in st.session_state:
    st.session_state.context = {}

if 'file_uploaded' not in st.session_state:
    st.session_state.file_uploaded = False

if 'config' not in st.session_state:
    st.session_state.config = {
        'max_synthetic_rows': 500,
        'analysis_mode': 'B√°sico',
        'enable_validation': True
    }

def limit_chat_history():
    """Limita el historial de chat para evitar exceso de tokens"""
    if len(st.session_state.chat_history) > 20:
        st.session_state.chat_history = st.session_state.chat_history[-20:]

def process_uploaded_file(uploaded_file=None):
    """Procesa un archivo cargado desde el sidebar"""
    if uploaded_file:
        try:
            with st.spinner(f"Procesando {uploaded_file.name}..."):
                if uploaded_file.name.endswith('.csv'):
                    df = pd.read_csv(uploaded_file)
                else:
                    df = pd.read_excel(uploaded_file)
                
                st.session_state.context.update({
                    'dataframe': df,
                    'dataset_uploaded': True,
                    'filename': uploaded_file.name,
                    'rows': df.shape[0],
                    'columns': df.shape[1]
                })
                
                st.session_state.file_uploaded = True
                st.session_state.uploaded_file = uploaded_file
                
                st.success(f"‚úÖ Archivo {uploaded_file.name} cargado exitosamente")
                
                st.session_state.chat_history.append({
                    "role": "assistant",
                    "content": f"üìä **Archivo cargado exitosamente**\n\n**{uploaded_file.name}**\n- {df.shape[0]:,} filas\n- {df.shape[1]} columnas\n\n¬øQu√© te gustar√≠a hacer con estos datos?",
                    "dataset_loaded": True,
                    "dataset_info": {
                        "filename": uploaded_file.name,
                        "rows": df.shape[0],
                        "columns": df.shape[1],
                        "dtypes": df.dtypes.value_counts().to_dict()
                    },
                    "dataset_preview": df.head().to_dict('records')
                })
                
                return True
                
        except Exception as e:
            st.error(f"Error al procesar el archivo: {e}")
            return False
    return False

def handle_synthetic_data_response(response, context=None):
    """Maneja la respuesta de generaci√≥n sint√©tica de forma centralizada"""
    if "synthetic_data" in response:
        synthetic_df = response["synthetic_data"]
        generation_info = response.get("generation_info", {})
        
        # Si generation_info est√° vac√≠o o incompleto, crear uno por defecto
        if not generation_info or not generation_info.get('model_type'):
            # Intentar extraer informaci√≥n del contexto o response
            context = context or {}
            parameters = context.get("parameters", {})
            
            # Crear generation_info por defecto con informaci√≥n disponible
            generation_info = {
                "model_type": parameters.get("model_type", "ctgan"),  # Modelo por defecto
                "num_samples": len(synthetic_df),
                "columns_used": len(synthetic_df.columns),
                "selection_method": "Columnas seleccionadas" if context.get('selected_columns') else "Autom√°tico",
                "timestamp": datetime.now().strftime('%Y%m%d_%H%M%S')
            }
        else:
            # Asegurar que tiene timestamp
            if 'timestamp' not in generation_info:
                generation_info["timestamp"] = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # Asegurar que num_samples coincide con los datos reales
            if 'num_samples' not in generation_info or generation_info['num_samples'] != len(synthetic_df):
                generation_info["num_samples"] = len(synthetic_df)
            
            # Asegurar que columns_used coincide con los datos reales
            if 'columns_used' not in generation_info or generation_info['columns_used'] != len(synthetic_df.columns):
                generation_info["columns_used"] = len(synthetic_df.columns)
        
        # Guardar en session_state
        st.session_state.context["synthetic_data"] = synthetic_df
        st.session_state.context["generation_info"] = generation_info
        
        return True
    return False

# Header principal con logo integrado
logo_path = os.path.join(project_root, "assets", "logo_patientia.png")
if os.path.exists(logo_path):
    col_left, col_logo, col_title, col_right = st.columns([5.7, 0.7, 6, 2])
    with col_logo:
        st.image(logo_path, width=54)
    with col_title:
        st.markdown(
            "<h1 style='font-size: 2.2rem; font-weight: 600; color: #1f2937; margin: 0; display: flex; align-items: center; height: 54px;'>Patient-IA</h1>",
            unsafe_allow_html=True
        )
else:
    st.markdown("""
    <div class="main-header">
        <h1>Patient-IA</h1>
    </div>
    """, unsafe_allow_html=True)

# Status indicator actualizado
if LLM_CONFIGURED and connection_test:
    if active_provider == "azure":
        status_text = "‚úÖ Azure OpenAI Conectado"
    elif active_provider == "ollama":
        status_text = "‚úÖ Ollama Local Conectado"
    elif active_provider == "grok":
        status_text = "‚úÖ Grok Conectado"
    else:
        status_text = "‚úÖ LLM Conectado"
    status_color = "#10b981"
elif LLM_CONFIGURED:
    status_text = f"üü° {active_provider.title()} Configurado (Sin conexi√≥n)"
    status_color = "#f59e0b"
else:
    status_text = "üîÑ Modo Simulado"
    status_color = "#6b7280"

st.markdown(f"""
<div class="status-indicator" style="border-color: {status_color}20; background: {status_color}10; color: {status_color};">
    <span>{status_text}</span>
    <span style="margin-left: 10px;">‚Ä¢</span>
    <span>{len(st.session_state.chat_history)} mensajes</span>
    {"<span style='margin-left: 10px;'>‚Ä¢ Archivo cargado</span>" if st.session_state.get('file_uploaded', False) else ""}
</div>
""", unsafe_allow_html=True)

# Sidebar mejorado con funcionalidades √∫tiles
with st.sidebar:
    st.header("Panel de Control")
    
    # Cargar Dataset
    st.subheader("Cargar Dataset")
    uploaded_file_sidebar = st.file_uploader(
        "Sube un archivo CSV o Excel", 
        type=["csv", "xlsx", "xls"], 
        key="sidebar_uploader"
    )
    
    # Procesar archivo si se ha subido uno nuevo
    if uploaded_file_sidebar is not None:
        current_file_key = f"{uploaded_file_sidebar.name}_{uploaded_file_sidebar.size}"
        previous_file_key = st.session_state.get('last_processed_file_key', '')
        
        if current_file_key != previous_file_key:
            st.session_state.last_processed_file_key = current_file_key
            if process_uploaded_file(uploaded_file_sidebar):
                st.rerun()
    
    # Informaci√≥n del dataset si est√° cargado
    if st.session_state.get('file_uploaded', False):
        st.subheader("Dataset Actual")
        filename = st.session_state.context.get("filename", "archivo")
        rows = st.session_state.context.get("rows", 0)
        cols = st.session_state.context.get("columns", 0)
        
        st.success(f"‚úÖ **{filename}**")
        st.info(f"üìà **{rows:,}** filas, **{cols}** columnas")
        
        # Bot√≥n para seleccionar columnas para generaci√≥n sint√©tica
        if COLUMN_SELECTOR_AVAILABLE and 'dataframe' in st.session_state.context:
            if st.button("üîç Seleccionar Columnas", use_container_width=True):
                st.session_state.show_column_selector = True
        
        # Bot√≥n para quitar archivo
        if st.button("Quitar Archivo", use_container_width=True):
            keys_to_reset = ['file_uploaded', 'uploaded_file', 'context', 'analysis_complete', 'show_column_selector', 'selected_columns']
            for key in keys_to_reset:
                if key in st.session_state:
                    del st.session_state[key]
            st.rerun()
        
        # Bot√≥n para nueva conversaci√≥n
        if st.button("Nueva Conversaci√≥n", use_container_width=True):
            for key in list(st.session_state.keys()):
                if key != 'orchestrator':
                    del st.session_state[key]
            st.rerun()
    
    # Comandos r√°pidos
    st.subheader("Comandos R√°pidos")
    if st.button("Analizar Datos", use_container_width=True):
        if st.session_state.get('file_uploaded'):
            st.session_state.quick_command = "analizar datos"
        else:
            st.warning("Primero carga un archivo")
    
    if st.button("ü§ñ Generar Sint√©ticos", use_container_width=True):
        if st.session_state.get('file_uploaded'):
            # Mostrar selector de modelos en lugar de quick_command
            st.session_state.show_model_selector = True
            st.session_state.selected_model = 'ctgan'  # Modelo por defecto
        else:
            st.warning("Primero carga un archivo")
    
    if st.button("Validar Datos", use_container_width=True):
        if st.session_state.get('file_uploaded'):
            st.session_state.quick_command = "validar datos"
        else:
            st.warning("Primero carga un archivo")
    
    if st.button("Evaluar Calidad", use_container_width=True):
        if st.session_state.get('file_uploaded'):
            st.session_state.quick_command = "evaluar calidad"
        else:
            st.warning("Primero carga un archivo")
    
    if st.button("Simular Paciente", use_container_width=True):
        if st.session_state.get('file_uploaded'):
            st.session_state.quick_command = "simular paciente"
        else:
            st.warning("Primero carga un archivo")
    
    # Configuraci√≥n avanzada
    with st.expander("Configuraci√≥n Avanzada"):
        st.markdown("**L√≠mites de Generaci√≥n:**")
        max_synthetic_rows = st.slider("M√°x. registros sint√©ticos", 50, 1000, 500)
        
        st.markdown("**Modo de An√°lisis:**")
        analysis_mode = st.selectbox("Tipo de an√°lisis", ["B√°sico", "Detallado", "Experto"])
        
        st.markdown("**Validaci√≥n:**")
        enable_validation = st.checkbox("Validaci√≥n autom√°tica", value=True)
        
        st.session_state.config = {
            'max_synthetic_rows': max_synthetic_rows,
            'analysis_mode': analysis_mode,
            'enable_validation': enable_validation
        }
    
    # Selector de modelo para generaci√≥n sint√©tica
    if st.session_state.get('show_model_selector', False):
        st.markdown("---")
        st.subheader("ü§ñ Selecci√≥n de Modelo de Generaci√≥n")
        
        # Explicaciones de modelos
        model_info = {
            'ctgan': {
                'name': 'CTGAN (Conditional Tabular GAN)',
                'description': 'Red neuronal generativa adversarial especializada en datos tabulares.',
                'pros': '‚Ä¢ Excelente para datos mixtos (categ√≥ricos + num√©ricos)\n‚Ä¢ Maneja correlaciones complejas\n‚Ä¢ R√°pido entrenamiento',
                'cons': '‚Ä¢ Puede generar outliers\n‚Ä¢ Requiere ajuste de hiperpar√°metros',
                'best_for': 'Datasets m√©dicos con variables categ√≥ricas y num√©ricas mezcladas',
                'color': 'blue'
            },
            'tvae': {
                'name': 'TVAE (Tabular Variational AutoEncoder)', 
                'description': 'Autoencoder variacional optimizado para datos tabulares.',
                'pros': '‚Ä¢ Preserva distribuciones estad√≠sticas\n‚Ä¢ Menos propenso a outliers\n‚Ä¢ Estable y confiable',
                'cons': '‚Ä¢ Puede ser conservador\n‚Ä¢ Menor diversidad en algunos casos',
                'best_for': 'Cuando se requiere alta fidelidad estad√≠stica',
                'color': 'green'
            },
            'sdv': {
                'name': 'SDV (Synthetic Data Vault)',
                'description': 'Suite completa de s√≠ntesis con m√∫ltiples algoritmos.',
                'pros': '‚Ä¢ Algoritmos m√∫ltiples integrados\n‚Ä¢ Optimizado para datos m√©dicos\n‚Ä¢ Validaci√≥n autom√°tica',
                'cons': '‚Ä¢ Mayor complejidad computacional\n‚Ä¢ Tiempo de entrenamiento m√°s largo',
                'best_for': 'Proyectos que requieren m√°xima calidad y validaci√≥n',
                'color': 'orange'
            }
        }
        
        # Selector de modelo por defecto
        default_model = st.session_state.get('selected_model', 'ctgan')
        
        # Crear tabs para cada modelo
        tab1, tab2, tab3 = st.tabs(['üîµ CTGAN', 'üü¢ TVAE', 'üü† SDV'])
        
        with tab1:
            info = model_info['ctgan']
            st.markdown(f"**{info['name']}**")
            st.write(info['description'])
            
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("**‚úÖ Ventajas:**")
                st.markdown(info['pros'])
            with col2:
                st.markdown("**‚ö†Ô∏è Consideraciones:**")
                st.markdown(info['cons'])
            
            st.info(f"**üéØ Ideal para:** {info['best_for']}")
            
            if st.button("Seleccionar CTGAN", key="select_ctgan", use_container_width=True):
                st.session_state.selected_model = 'ctgan'
                st.success("‚úÖ CTGAN seleccionado")
        
        with tab2:
            info = model_info['tvae']
            st.markdown(f"**{info['name']}**")
            st.write(info['description'])
            
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("**‚úÖ Ventajas:**")
                st.markdown(info['pros'])
            with col2:
                st.markdown("**‚ö†Ô∏è Consideraciones:**")
                st.markdown(info['cons'])
            
            st.info(f"**üéØ Ideal para:** {info['best_for']}")
            
            if st.button("Seleccionar TVAE", key="select_tvae", use_container_width=True):
                st.session_state.selected_model = 'tvae'
                st.success("‚úÖ TVAE seleccionado")
        
        with tab3:
            info = model_info['sdv']
            st.markdown(f"**{info['name']}**")
            st.write(info['description'])
            
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("**‚úÖ Ventajas:**")
                st.markdown(info['pros'])
            with col2:
                st.markdown("**‚ö†Ô∏è Consideraciones:**")
                st.markdown(info['cons'])
            
            st.info(f"**üéØ Ideal para:** {info['best_for']}")
            
            if st.button("Seleccionar SDV", key="select_sdv", use_container_width=True):
                st.session_state.selected_model = 'sdv'
                st.success("‚úÖ SDV seleccionado")
        
        st.markdown("---")
        
        # Configuraci√≥n adicional
        col1, col2 = st.columns(2)
        with col1:
            num_samples = st.number_input("N√∫mero de registros a generar", min_value=10, max_value=1000, value=100, step=10)
        with col2:
            st.markdown("**Modelo seleccionado:**")
            selected_model = st.session_state.get('selected_model', 'ctgan')
            st.markdown(f"ü§ñ **{model_info[selected_model]['name']}**")
        
        # Botones de acci√≥n
        col1, col2, col3 = st.columns([1, 1, 1])
        
        with col1:
            if st.button("üöÄ Generar Datos", use_container_width=True):
                # Preparar contexto para generaci√≥n
                context_for_generation = st.session_state.context.copy()
                context_for_generation['parameters'] = {
                    'model_type': st.session_state.get('selected_model', 'ctgan'),
                    'num_samples': num_samples
                }
                
                # A√±adir columnas seleccionadas si existen
                if st.session_state.get('selected_columns'):
                    context_for_generation['selected_columns'] = st.session_state.selected_columns
                
                # Simular llamada al generador
                prompt_for_generation = f"Genera {num_samples} registros sint√©ticos usando el modelo {selected_model.upper()}"
                st.session_state.pending_generation = {
                    'prompt': prompt_for_generation,
                    'context': context_for_generation
                }
                st.session_state.show_model_selector = False
                st.success(f"‚úÖ Configuraci√≥n guardada. Generando con {selected_model.upper()}...")
                st.rerun()
        
        with col2:
            if st.button("üîÑ Cambiar Modelo", use_container_width=True):
                st.session_state.selected_model = 'ctgan'  # Reset a default
                st.rerun()
        
        with col3:
            if st.button("‚ùå Cancelar", use_container_width=True):
                st.session_state.show_model_selector = False
                if 'selected_model' in st.session_state:
                    del st.session_state.selected_model
                st.rerun()

    # Informaci√≥n de datos sint√©ticos generados
    if st.session_state.context.get('synthetic_data') is not None:
        st.markdown("---")
        st.subheader("üìä Datos Sint√©ticos Generados")
        
        synthetic_df = st.session_state.context['synthetic_data']
        generation_info = st.session_state.context.get('generation_info', {})
        
        # M√©tricas b√°sicas
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Registros", f"{len(synthetic_df):,}")
        with col2:
            st.metric("Columnas", len(synthetic_df.columns))
        with col3:
            # Mostrar informaci√≥n del modelo de forma m√°s clara
            model_type = generation_info.get('model_type', 'N/A')
            if model_type and model_type != 'N/A':
                model_display = model_type.upper()
            else:
                # Si no tenemos info del modelo, mostrar informaci√≥n del DataFrame
                model_display = "GENERADO"
            st.metric("Modelo", model_display)
        
        # Vista previa de los datos
        st.markdown("**üîç Vista previa de datos sint√©ticos:**")
        st.dataframe(synthetic_df.head(10), use_container_width=True)
        
        # Informaci√≥n de generaci√≥n
        if generation_info:
            with st.expander("üî¨ Detalles de generaci√≥n"):
                col1, col2 = st.columns(2)
                with col1:
                    # Formatear modelo de forma segura
                    model_type = generation_info.get('model_type', 'N/A')
                    if model_type and model_type != 'N/A':
                        st.text(f"Modelo utilizado: {model_type.upper()}")
                    else:
                        st.text(f"Modelo utilizado: Datos sint√©ticos generados")
                    # Formatear n√∫mero de muestras de forma segura
                    num_samples = generation_info.get('num_samples', len(synthetic_df))
                    if isinstance(num_samples, (int, float)):
                        st.text(f"Registros generados: {int(num_samples):,}")
                    else:
                        st.text(f"Registros generados: {len(synthetic_df):,}")
                with col2:
                    selection_method = generation_info.get('selection_method', 'N/A')
                    if selection_method == 'N/A':
                        selection_method = "M√©todo est√°ndar"
                    st.text(f"M√©todo de selecci√≥n: {selection_method}")
                    
                    columns_used = generation_info.get('columns_used', len(synthetic_df.columns))
                    st.text(f"Columnas utilizadas: {columns_used}")
        else:
            # Si no hay generation_info, mostrar informaci√≥n b√°sica del DataFrame
            with st.expander("üî¨ Informaci√≥n de los datos"):
                col1, col2 = st.columns(2)
                with col1:
                    st.text(f"Datos sint√©ticos: Generados exitosamente")
                    st.text(f"Registros: {len(synthetic_df):,}")
                with col2:
                    st.text(f"Columnas: {len(synthetic_df.columns)}")
                    st.text(f"M√©todo: Generaci√≥n est√°ndar")
        
        # Botones de descarga mejorados
        col1, col2, col3 = st.columns([1, 1, 1])
        
        with col1:
            csv_data = synthetic_df.to_csv(index=False)
            timestamp = generation_info.get('timestamp', datetime.now().strftime('%Y%m%d_%H%M%S'))
            filename_csv = f"datos_sinteticos_{generation_info.get('model_type', 'ctgan')}_{len(synthetic_df)}_{timestamp}.csv"
            st.download_button(
                label="üìÑ Descargar CSV",
                data=csv_data,
                file_name=filename_csv,
                mime="text/csv",
                use_container_width=True
            )
        
        with col2:
            json_data = synthetic_df.to_json(orient='records', indent=2)
            filename_json = f"datos_sinteticos_{generation_info.get('model_type', 'ctgan')}_{len(synthetic_df)}_{timestamp}.json"
            st.download_button(
                label="üìã Descargar JSON",
                data=json_data,
                file_name=filename_json,
                mime="application/json",
                use_container_width=True
            )
        
        with col3:
            # Bot√≥n para limpiar datos sint√©ticos
            if st.button("üóëÔ∏è Limpiar", use_container_width=True, help="Limpiar datos sint√©ticos generados"):
                del st.session_state.context['synthetic_data']
                if 'generation_info' in st.session_state.context:
                    del st.session_state.context['generation_info']
                st.success("‚úÖ Datos sint√©ticos eliminados")
                st.rerun()


        

        
    # Agregar nota de seguridad y privacidad
    st.markdown("""
    ---
    **üîí Privacidad y Seguridad:** Todos los datos se procesan localmente. Los datos sint√©ticos mantienen las propiedades estad√≠sticas 
    del dataset original mientras protegen la identidad de los pacientes individuales.
    """)
    
    # Mostrar estado del sistema en la parte inferior
    st.subheader("üîß Estado del Sistema")
    status_col1, status_col2, status_col3 = st.columns(3)
    with status_col1:
        if LLM_CONFIGURED and connection_test:
            if active_provider == "azure":
                llm_status = "üü¢ Azure Conectado"
            elif active_provider == "ollama":
                llm_status = "üü¢ Ollama Local"
            elif active_provider == "grok":
                llm_status = "üü¢ Grok Conectado"
            else:
                llm_status = "üü¢ LLM Conectado"
        else:
            llm_status = "üü° Modo Simulado"
        st.info(f"**Proveedor LLM:** {llm_status}")
    
    with status_col2:
        # Detectar tipo de orquestador
        orchestrator_type = type(st.session_state.get('orchestrator', None)).__name__
        if "FastMedical" in orchestrator_type:
            orchestrator_status = "üöÄ Fast Mode"
            orchestrator_color = "green"
        elif "MedicalAgents" in orchestrator_type:
            orchestrator_status = "üîß Full Mode" 
            orchestrator_color = "blue"
        else:
            orchestrator_status = "üü° Mock Mode"
            orchestrator_color = "orange"
        st.info(f"**Orquestador:** {orchestrator_status}")
        
    with status_col3:
        agents_status = "üü¢ Disponibles" if AGENTS_AVAILABLE else "üü° Mock Agents"
        st.info(f"**Agentes IA:** {agents_status}")
    
    # Mostrar informaci√≥n adicional del orquestador en uso
    if "FastMedical" in orchestrator_type:
        st.success("‚ö° **Modo Fast**: Respuestas optimizadas sin timeout. Agentes disponibles para tareas espec√≠ficas.")
    elif "MedicalAgents" in orchestrator_type:
        st.info("üîß **Modo Full**: Workflow completo con LangGraph. Todas las capacidades disponibles.")
    else:
        st.warning("üü° **Modo Mock**: Respuestas simuladas para desarrollo. Configura LLM para funcionalidad completa.")

with st.container():
    # Mostrar historial de chat
    if st.session_state.chat_history:
        for i, message in enumerate(st.session_state.chat_history):
            with st.chat_message(message["role"]):
                # Limpiar el mensaje de caracteres Unicode y JSON crudo
                clean_content = clean_response_message(message["content"])
                st.markdown(clean_content)
                
                # MOSTRAR INFORMACI√ìN ESPECIAL DEL DATASET
                if message.get("dataset_loaded"):
                    dataset_info = message.get("dataset_info", {})
                    dataset_preview = message.get("dataset_preview", [])
                    
                    with st.expander("üëÅÔ∏è Vista previa del dataset", expanded=False):
                        if dataset_preview:
                            preview_df = pd.DataFrame(dataset_preview)
                            st.dataframe(preview_df, use_container_width=True)
                        
                        col1, col2 = st.columns(2)
                        with col1:
                            st.markdown("**üìä Distribuci√≥n de tipos:**")
                            for dtype, count in dataset_info.get('dtypes', {}).items():
                                st.text(f"{dtype}: {count} columnas")
                        
                        with col2:
                            st.markdown("**üìã Informaci√≥n t√©cnica:**")
                            st.text(f"Nombre: {dataset_info.get('filename', 'N/A')}")
                            st.text(f"Filas: {dataset_info.get('rows', 0):,}")
                            st.text(f"Columnas: {dataset_info.get('columns', 0)}")
    else:
        # Interfaz est√°tica de bienvenida (NO es un mensaje de chat)
        st.markdown("""
        <div style="text-align: center; margin-bottom: 2rem;">
            <h1 style="font-size: 2.0rem; font-weight: 600; color: #1f2937; margin-bottom: 0.5rem;">
                Asistente Inteligente para Generaci√≥n de Datos Cl√≠nicos Sint√©ticos
            </h1>
            
        </div>
        """, unsafe_allow_html=True)

        # Crear dos columnas principales para explicar las capacidades del sistema
        col1, col2 = st.columns(2, gap="large")
        
        with col1:
            st.markdown("""
            ### ÔøΩ **Capacidades del Sistema**
            
            **üß† Agentes Especializados:**
            - **Analista Cl√≠nico** - Extrae patrones y estad√≠sticas m√©dicas
            - **Generador Sint√©tico** - Crea datos realistas con CTGAN/TVAE/SDV
            - **Validador M√©dico** - Verifica coherencia cl√≠nica
            - **Simulador de Pacientes** - Modela evoluci√≥n temporal
            - **Evaluador de Utilidad** - Mide calidad y privacidad
            
            **üìä Formatos Soportados:**
            - CSV, Excel (.xlsx, .xls)
            - Datos COVID-19, oncol√≥gicos, cardiol√≥gicos
            - Historiales cl√≠nicos y registros m√©dicos
            """)
        
        with col2:
            st.markdown("""
            ### üöÄ **C√≥mo Empezar**
            
            **1Ô∏è‚É£ Cargar Datos:**
            - Sube tu archivo CSV/Excel con datos cl√≠nicos
            - El sistema detectar√° autom√°ticamente el dominio m√©dico
            - Selecciona las columnas m√°s relevantes (opcional)
            
            **2Ô∏è‚É£ Interactuar:**
            - **"Analiza estos datos"** - Para explorar patrones
            - **"Genera 1000 muestras con CTGAN"** - Para crear sint√©ticos
            - **"Valida la coherencia m√©dica"** - Para verificar calidad
            - **"¬øCu√°les son los factores de riesgo cardiovascular?"** - Consultas m√©dicas
            
            **3Ô∏è‚É£ Descargar:**
            - Descarga los datos sint√©ticos en CSV/JSON
            - Revisa m√©tricas de calidad y privacidad
            """)

# --- L√ìGICA PRINCIPAL DEL CHAT ---
def main_chat_loop():
    """Funci√≥n principal para manejar el bucle del chat usando wrapper s√≠ncrono."""
    limit_chat_history()
    
    # Procesar comandos r√°pidos del sidebar
    if hasattr(st.session_state, 'quick_command'):
        prompt = st.session_state.quick_command
        del st.session_state.quick_command
        
        st.session_state.chat_history.append({"role": "user", "content": prompt})
        
        with st.spinner("Procesando comando..."):
            # Preparar contexto incluyendo columnas seleccionadas si existen
            context_with_selections = st.session_state.context.copy()
            if st.session_state.get('selected_columns'):
                context_with_selections['selected_columns'] = st.session_state.selected_columns
            
            response = process_orchestrator_input_safe(st.session_state.orchestrator, prompt, context_with_selections)
            full_response = response.get("message", "No se recibi√≥ respuesta.")
            
            # Limpiar la respuesta antes de guardarla en el historial
            clean_response = clean_response_message(full_response)
            
            st.session_state.chat_history.append({
                "role": "assistant", 
                "content": clean_response, 
                "agent": response.get("agent"),
                "dataset_type": response.get("dataset_type")
            })
        
        st.rerun()
        return
    
    # JavaScript para indentaci√≥n autom√°tica
    st.markdown("""
    <script>
        function setupTextIndentation() {
            const textarea = document.querySelector('.stChatInput textarea');
            
            if (textarea) {
                const tenSpaces = '          '; // 10 espacios
                
                textarea.addEventListener('focus', function() {
                    if (this.value === '' || this.value === tenSpaces) {
                        this.value = tenSpaces;
                        this.setSelectionRange(tenSpaces.length, tenSpaces.length);
                    }
                });
                
                textarea.addEventListener('input', function() {
                    if (!this.value.startsWith(tenSpaces)) {
                        const cursorPos = this.selectionStart;
                        const restOfText = this.value.replace(/^\\s*/, '');
                        this.value = tenSpaces + restOfText;
                        this.setSelectionRange(Math.max(tenSpaces.length, cursorPos), Math.max(tenSpaces.length, cursorPos));
                    }
                });
                
                textarea.addEventListener('keydown', function(e) {
                    if (e.key === 'Backspace' && this.selectionStart <= tenSpaces.length) {
                        e.preventDefault();
                    }
                    if (e.key === 'Delete' && this.selectionStart < tenSpaces.length) {
                        e.preventDefault();
                    }
                    if (e.key === 'Home') {
                        e.preventDefault();
                        this.setSelectionRange(tenSpaces.length, tenSpaces.length);
                    }
                });
            }
        }
        
        document.addEventListener('DOMContentLoaded', function() {
            setTimeout(setupTextIndentation, 500);
            setTimeout(setupTextIndentation, 1500);
            setTimeout(setupTextIndentation, 3000);
        });
    </script>
    """, unsafe_allow_html=True)
    
    # Interfaz de selecci√≥n de columnas
    if st.session_state.get('show_column_selector', False) and COLUMN_SELECTOR_AVAILABLE:
        st.markdown("---")
        
        # Crear instancia del selector de columnas
        column_selector = MedicalColumnSelector()
        df = st.session_state.context.get('dataframe')
        
        if df is not None:
            # Validar primero si el dataset es m√©dico v√°lido
            is_valid, validation_errors = column_selector.validate_medical_dataset(df)
            
            if not is_valid:
                st.error("‚ùå **Dataset no v√°lido para generaci√≥n sint√©tica m√©dica**")
                for error in validation_errors:
                    st.warning(f"‚ö†Ô∏è {error}")
                
                if st.button("Cerrar Selector"):
                    st.session_state.show_column_selector = False
                    st.rerun()
            else:
                # Mostrar interfaz de selecci√≥n
                column_selection = column_selector.generate_column_selection_interface(df)
                
                # Botones de acci√≥n
                col1, col2, col3 = st.columns([1, 1, 1])
                
                with col1:
                    if st.button("‚úÖ Confirmar Selecci√≥n", use_container_width=True, disabled=not column_selection.mandatory_fulfilled):
                        st.session_state.selected_columns = column_selection.selected_columns
                        st.session_state.show_column_selector = False
                        st.success(f"Seleccionadas {len(column_selection.selected_columns)} columnas para generaci√≥n sint√©tica")
                        st.rerun()
                
                with col2:
                    if st.button("üîÑ Usar Recomendadas", use_container_width=True):
                        # Usar selecci√≥n autom√°tica recomendada
                        dataset_type = column_selector.detector.detect_dataset_type(df)
                        column_mappings = column_selector.detector.infer_medical_columns(df)
                        recommended = column_selector._get_recommended_columns(dataset_type, column_mappings, df)
                        
                        st.session_state.selected_columns = recommended
                        st.session_state.show_column_selector = False
                        st.success(f"Usando {len(recommended)} columnas recomendadas autom√°ticamente")
                        st.rerun()
                
                with col3:
                    if st.button("‚ùå Cancelar", use_container_width=True):
                        st.session_state.show_column_selector = False
                        st.rerun()
        
        st.markdown("---")
    
    # Mostrar columnas seleccionadas si existen
    if st.session_state.get('selected_columns'):
        columns_display = st.session_state.selected_columns[:5]
        more_text = f" (+{len(st.session_state.selected_columns)-5} m√°s)" if len(st.session_state.selected_columns) > 5 else ""
        
        col1, col2 = st.columns([3, 1])
        with col1:
            st.info(f"üéØ **Columnas seleccionadas:** {', '.join(columns_display)}{more_text}")
        with col2:
            if st.button("‚ùå Limpiar selecci√≥n", help="Eliminar selecci√≥n de columnas"):
                del st.session_state.selected_columns
                st.rerun()
    
    # Procesar generaci√≥n pendiente si existe
    if st.session_state.get('pending_generation'):
        pending = st.session_state.pending_generation
        del st.session_state.pending_generation
        
        # A√±adir el prompt de generaci√≥n al historial
        st.session_state.chat_history.append({"role": "user", "content": pending['prompt']})
        
        with st.spinner("üîÑ Generando datos sint√©ticos..."):
            response = process_orchestrator_input_safe(
                st.session_state.orchestrator,
                pending['prompt'], 
                pending['context']
            )
            
            # Manejar respuesta de generaci√≥n sint√©tica
            handle_synthetic_data_response(response, pending['context'])
            
            full_response = response.get("message", "No se recibi√≥ respuesta.")
            
            # Limpiar la respuesta antes de guardarla en el historial
            clean_response = clean_response_message(full_response)
            
            st.session_state.chat_history.append({
                "role": "assistant", 
                "content": clean_response, 
                "agent": response.get("agent"),
                "dataset_type": response.get("dataset_type")
            })
        
        st.rerun()
        return

    # Chat input principal de Streamlit
    if prompt := st.chat_input("¬øC√≥mo puedo ayudarte?"):
        st.session_state.chat_history.append({"role": "user", "content": prompt})
        
        # Crear un placeholder para el progreso
        progress_placeholder = st.empty()
        
        with st.spinner("üîÑ Procesando solicitud..."):
            # Preparar contexto incluyendo columnas seleccionadas si existen
            context_with_selections = st.session_state.context.copy()
            if st.session_state.get('selected_columns'):
                context_with_selections['selected_columns'] = st.session_state.selected_columns
                progress_placeholder.info(f"‚úÖ Usando {len(st.session_state.selected_columns)} columnas seleccionadas")
            
            response = process_orchestrator_input_safe(st.session_state.orchestrator, prompt, context_with_selections)
            progress_placeholder.empty()  # Limpiar el mensaje de progreso
            
            # Manejar respuesta de generaci√≥n sint√©tica
            handle_synthetic_data_response(response, context_with_selections)
            
            if response.get("error"):
                error_message = response.get("error")
                if isinstance(error_message, bool):
                    error_message = "Error en el procesamiento del dataset"
                elif not isinstance(error_message, str):
                    error_message = str(error_message)
                
                st.error(f"‚ùå **Error**: {error_message}")
                return
            
            # Obtener y limpiar el mensaje de respuesta
            raw_message = response.get("message", "No se recibi√≥ respuesta.")
            
            # Limpiar caracteres Unicode escapados y JSON crudo si aparece
            cleaned_message = clean_response_message(raw_message)
            
            st.session_state.chat_history.append({
                "role": "assistant", 
                "content": cleaned_message, 
                "agent": response.get("agent"),
                "dataset_type": response.get("dataset_type")
            })
        
        st.rerun()

def process_orchestrator_input_safe(orchestrator, user_input: str, context: dict = None):
    """
    Ejecuta el orquestador de forma s√≠ncrona usando el wrapper para evitar problemas de event loop.
    """
    try:
        print(f"üîç Debugging orchestrator type: {type(orchestrator)}")
        print(f"üîç Has process_user_input_sync: {hasattr(orchestrator, 'process_user_input_sync')}")
        print(f"üîç Has process_user_input: {hasattr(orchestrator, 'process_user_input')}")
        print(f"üîç ASYNC_WRAPPER_AVAILABLE: {ASYNC_WRAPPER_AVAILABLE}")
        
        # Verificar si el orquestador tiene el m√©todo s√≠ncrono
        if hasattr(orchestrator, 'process_user_input_sync'):
            print("‚úÖ Usando process_user_input_sync")
            return orchestrator.process_user_input_sync(user_input, context)
        
        # Si no tiene el m√©todo s√≠ncrono pero tenemos el wrapper disponible
        elif ASYNC_WRAPPER_AVAILABLE and hasattr(orchestrator, 'process_user_input'):
            print("‚úÖ Usando wrapper con process_user_input")
            # Pasar la funci√≥n y los argumentos por separado al wrapper
            return run_async_safe(orchestrator.process_user_input, user_input, context)
        
        # Fallback: intentar ejecutar directamente (para mocks)
        else:
            print("‚ö†Ô∏è Usando fallback directo")
            # Para orquestadores mock que pueden ser s√≠ncronos o async
            result = orchestrator.process_user_input(user_input, context)
            print(f"üîç Result type: {type(result)}")
            
            # Si es una corrutina, necesitamos ejecutarla de forma async
            import inspect
            if inspect.iscoroutine(result):
                print("üîç Result is coroutine, handling async")
                if ASYNC_WRAPPER_AVAILABLE:
                    # Cancelar la corrutina actual y crear una nueva llamada
                    result.close()  # Liberar la corrutina no ejecutada
                    return run_async_safe(orchestrator.process_user_input, user_input, context)
                else:
                    print("‚ö†Ô∏è No wrapper available, using basic asyncio")
                    # Fallback simple: intentar con asyncio b√°sico
                    import asyncio
                    import concurrent.futures
                    
                    def run_async():
                        new_loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(new_loop)
                        try:
                            return new_loop.run_until_complete(orchestrator.process_user_input(user_input, context))
                        finally:
                            new_loop.close()
                    
                    try:
                        with concurrent.futures.ThreadPoolExecutor() as executor:
                            future = executor.submit(run_async)
                            return future.result(timeout=30)
                    except Exception as e:
                        print(f"Error en fallback async: {e}")
                        return {
                            "message": f"‚ùå Error ejecutando operaci√≥n async: {str(e)}",
                            "agent": "system", 
                            "error": True
                        }
            else:
                print("‚úÖ Result is sync, returning directly")
                # Es s√≠ncrono, devolver directamente
                return result
            
    except Exception as e:
        print(f"‚ùå Error en process_orchestrator_input_safe: {e}")
        import traceback
        traceback.print_exc()
        return {
            "message": f"‚ùå Error interno: {str(e)}",
            "agent": "system", 
            "error": True
        }

# Funci√≥n initialize_orchestrator ya definida arriba

# Inicializaci√≥n del estado ya definida arriba

# Ejecutar chat usando el wrapper s√≠ncrono
if __name__ == "__main__":
    # Como main_chat_loop ahora es s√≠ncrono, llamarlo directamente
    try:
        main_chat_loop()
    except Exception as e:
        st.error(f"‚ùå Error en el bucle principal del chat: {e}")
        st.write("‚ö†Ô∏è Por favor, reinicia la aplicaci√≥n")
else:
    # Si no estamos en main, ejecutar el bucle normalmente (para Streamlit)
    main_chat_loop()
