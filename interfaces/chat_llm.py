import streamlit as st
import pandas as pd
import numpy as np
import asyncio
import os
import sys
import re
from datetime import datetime
from dotenv import load_dotenv

# A√±adir la ruta del proyecto al path de Python
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# Cargar variables de entorno
load_dotenv()

# Importar el selector de columnas m√©dicas
try:
    from src.adapters.medical_column_selector import MedicalColumnSelector
    COLUMN_SELECTOR_AVAILABLE = True
except ImportError as e:
    COLUMN_SELECTOR_AVAILABLE = False
    print(f"‚ö†Ô∏è Selector de columnas no disponible: {e}")

# Configuraci√≥n de p√°gina
st.set_page_config(
    page_title="Patient IA",
    page_icon=os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "assets", "logo_patientia.png"),
    layout="wide",
    initial_sidebar_state="collapsed"
)

# CSS personalizado para un dise√±o moderno
st.markdown("""
<style>
    /* Ocultar elementos innecesarios */
    .stDeployButton {display:none;}
    footer {visibility: hidden;}
    .stMainBlockContainer {padding-top: 2rem;}
    
    /* Estilo del chat */
    .chat-container {
        max-width: 800px;
        margin: 0 auto;
        padding: 0 1rem;
    }
    
    /* Prompt input m√°s grande */
    .stChatInput {
        position: relative;
        max-width: 700px !important;
        margin: 0 auto !important;
    }
    
    .stChatInput > div > div > textarea {
        min-height: 60px !important;
        font-size: 16px !important;
        border-radius: 25px !important;
        border: 2px solid #e1e5e9 !important;
        padding: 15px 75px 15px 60px !important;
        text-indent: 10ch !important;
    }
    
    /* Header minimalista */
    .main-header {
        text-align: center;
        margin-bottom: 0.5rem;
        padding: 1rem 0 0 0;
    }
    
    .main-header h1 {
        font-size: 2.5rem;
        font-weight: 600;
        color: #1f2937;
        margin-bottom: 0.5rem;
    }
    
    /* Logo centrado */
    .logo-container {
        text-align: center;
        margin-bottom: 0.5rem;
    }
    
    .logo-container img {
        margin: 0 auto;
        display: block;
    }
    
    /* Status indicator centrado */
    .status-indicator {
        display: flex;
        justify-content: center;
        align-items: center;
        gap: 8px;
        background: #f0f9ff;
        border: 1px solid #bfdbfe;
        border-radius: 20px;
        padding: 6px 12px;
        font-size: 0.875rem;
        color: #1e40af;
        margin: 0.5rem auto 1rem auto;
        width: fit-content;
    }
    
    /* Mensaje de bienvenida centrado */
    .welcome-container {
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        min-height: 40vh;
        margin-top: 0;
    }
    
    /* Mensajes del chat */
    .stChatMessage {
        margin-bottom: 1rem;
    }
</style>
""", unsafe_allow_html=True)

# Importar configuraci√≥n de Azure
try:
    from src.config.azure_config import azure_config
    AZURE_CONFIGURED = True
    try:
        connection_test = azure_config.test_connection()
        print("‚úÖ Azure OpenAI conectado correctamente")
    except Exception as e:
        connection_test = False
        print(f"‚ö†Ô∏è Azure configurado pero sin conexi√≥n: {e}")
except Exception as e:
    AZURE_CONFIGURED = False
    connection_test = False
    print(f"‚ö†Ô∏è Error de configuraci√≥n Azure: {e}")

# Importar LangGraph Orchestrator y agentes con manejo de errores
try:
    from src.orchestration.langgraph_orchestrator import MedicalAgentsOrchestrator, AgentState
    from src.agents.base_agent import BaseLLMAgent, BaseAgentConfig
    from src.agents.coordinator_agent import CoordinatorAgent
    from src.agents.analyzer_agent import ClinicalAnalyzerAgent
    from src.agents.generator_agent import SyntheticGeneratorAgent
    
    # Importar otros agentes con manejo individual de errores
    try:
        from src.agents.validator_agent import MedicalValidatorAgent
        VALIDATOR_AVAILABLE = True
    except Exception as e:
        print(f"‚ö†Ô∏è Validator agent no disponible: {e}")
        VALIDATOR_AVAILABLE = False
    
    try:
        from src.agents.simulator_agent import PatientSimulatorAgent
        SIMULATOR_AVAILABLE = True
    except Exception as e:
        print(f"‚ö†Ô∏è Simulator agent no disponible: {e}")
        SIMULATOR_AVAILABLE = False
    
    try:
        from src.agents.evaluator_agent import UtilityEvaluatorAgent
        EVALUATOR_AVAILABLE = True
    except Exception as e:
        print(f"‚ö†Ô∏è Evaluator agent no disponible: {e}")
        EVALUATOR_AVAILABLE = False

    AGENTS_AVAILABLE = True
    LANGGRAPH_AVAILABLE = True
    print("‚úÖ LangGraph Orchestrator y Agentes cargados correctamente")
    
except Exception as e:
    AGENTS_AVAILABLE = False
    LANGGRAPH_AVAILABLE = False
    VALIDATOR_AVAILABLE = False
    SIMULATOR_AVAILABLE = False
    EVALUATOR_AVAILABLE = False
    print(f"‚ùå Error cargando agentes: {e}")

# Agente mock para desarrollo
class MockAgent:
    def __init__(self, name):
        self.name = name
        self.config = type('Config', (), {'name': name})()
    
    async def process(self, input_text, context=None):
        context = context or {}
        has_dataset = context.get("dataset_uploaded", False)
        
        if "coordinador" in self.name.lower():
            dataset_msg = ""
            if has_dataset:
                filename = context.get("filename", "archivo")
                rows = context.get("rows", 0)
                cols = context.get("columns", 0)
                dataset_msg = f"\n\nDataset detectado: {filename} ({rows:,} filas, {cols} columnas)"
            
            return {
                "message": f"üëã **¬°Hola!** Soy tu asistente de IA para generar datos cl√≠nicos sint√©ticos.\n\nüî¨ **Estado:** {'‚úÖ Azure OpenAI Conectado' if connection_test else 'üîÑ Modo Simulado'}{dataset_msg}\n\n**üß† Mi equipo especializado:**\n‚Ä¢ **Analista** - Extrae patrones cl√≠nicos\n‚Ä¢ **Generador** - Crea datos sint√©ticos con SDV\n‚Ä¢ **Validador** - Verifica coherencia m√©dica\n‚Ä¢ **Simulador** - Modela evoluci√≥n temporal\n‚Ä¢ **Evaluador** - Mide calidad y utilidad\n\n¬øEn qu√© puedo ayudarte hoy?",
                "agent": self.name,
                "mock": True
            }
        elif "analista" in self.name.lower():
            if has_dataset:
                filename = context.get("filename", "dataset")
                rows = context.get("rows", 0)
                cols = context.get("columns", 0)
                return {
                    "message": f"""üîç **An√°lisis completado (modo simulado)**

**üìä Dataset analizado:** {filename}
- **Registros:** {rows:,}
- **Columnas:** {cols}

**üìà An√°lisis estad√≠stico:**
- Tipos de datos identificados
- Valores faltantes detectados  
- Distribuciones analizadas
- Correlaciones calculadas

**üí° Insights principales:**
- Dataset preparado para generaci√≥n sint√©tica
- Calidad de datos: Buena
- Recomendaci√≥n: Proceder con generaci√≥n TVAE o CTGAN

*Nota: An√°lisis completo disponible con Azure OpenAI configurado.*""",
                    "agent": self.name,
                    "mock": True
                }
            else:
                return {
                    "message": "üìÅ **No hay dataset cargado**\n\nPara an√°lisis cl√≠nico, necesito que subas un archivo CSV o Excel con datos m√©dicos.\n\nüìä **Formatos aceptados:** CSV, XLSX, XLS",
                    "agent": self.name,
                    "mock": True
                }
        elif "generador" in self.name.lower():
            if has_dataset:
                # Obtener par√°metros de generaci√≥n
                params = context.get("parameters", {})
                model_type = params.get("model_type", "ctgan")
                num_samples = params.get("num_samples", 100)
                
                # Crear datos sint√©ticos simulados
                original_df = context.get("dataframe")
                if original_df is not None:
                    # Crear una muestra sint√©tica usando el DataFrame original
                    synthetic_data = original_df.sample(n=min(num_samples, len(original_df)), replace=True).reset_index(drop=True)
                    
                    # A√±adir algo de ruido para simular diferencias
                    import numpy as np
                    for col in synthetic_data.select_dtypes(include=[np.number]).columns:
                        noise = np.random.normal(0, synthetic_data[col].std() * 0.05, len(synthetic_data))
                        synthetic_data[col] = synthetic_data[col] + noise
                    
                    return {
                        "message": f"""üß¨ **Generaci√≥n sint√©tica completada**

**üìä Resultado:**
- **Modelo utilizado:** {model_type.upper()}
- **Registros generados:** {len(synthetic_data):,}
- **Dataset base:** {len(original_df):,} registros

**‚úÖ Calidad de datos:** Los datos sint√©ticos mantienen las propiedades estad√≠sticas del dataset original mientras preservan la privacidad.""",
                        "agent": self.name,
                        "synthetic_data": synthetic_data,
                        "generation_info": {
                            "model_type": model_type,
                            "num_samples": len(synthetic_data),
                            "columns_used": len(synthetic_data.columns),
                            "selection_method": "Mock Generation",
                            "timestamp": pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')
                        }
                    }
                else:
                    rows = context.get("rows", 0)
                    return {
                        "message": f"""üß¨ **Generaci√≥n sint√©tica completada (modo simulado)**

**üìä Dataset base:** {rows:,} registros
**üéØ Registros generados:** {num_samples} (simulado)
**üî¨ Modelo utilizado:** {model_type.upper()} (simulado)

**‚úÖ Proceso completado:**
- Datos sint√©ticos generados exitosamente
- Calidad preservada
- Privacidad garantizada

*Nota: Generaci√≥n real disponible con Azure OpenAI configurado.*""",
                        "agent": self.name,
                        "mock": True
                    }
            else:
                return {
                    "message": "üìÅ **Dataset requerido**\n\nPara generar datos sint√©ticos, primero necesito un dataset base.\n\n**Sube un archivo** con datos cl√≠nicos para comenzar.",
                    "agent": self.name,
                    "mock": True
                }
        else:
            return {
                "message": f"ü§ñ **{self.name}**\n\n{input_text}\n\n*Funcionalidad completa disponible con Azure OpenAI configurado.*",
                "agent": self.name,
                "mock": True
            }

@st.cache_resource
def initialize_langgraph_orchestrator():
    """Inicializa el orquestador LangGraph con agentes"""
    if AGENTS_AVAILABLE and LANGGRAPH_AVAILABLE and AZURE_CONFIGURED:
        try:
            agents = {
                "coordinator": CoordinatorAgent(),
                "analyzer": ClinicalAnalyzerAgent(),
                "generator": SyntheticGeneratorAgent(),
            }
            
            # Agregar agentes opcionales solo si est√°n disponibles
            if VALIDATOR_AVAILABLE:
                agents["validator"] = MedicalValidatorAgent()
                print("‚úÖ Validator agent agregado")
            
            if SIMULATOR_AVAILABLE:
                agents["simulator"] = PatientSimulatorAgent()
                print("‚úÖ Simulator agent agregado")
            
            if EVALUATOR_AVAILABLE:
                agents["evaluator"] = UtilityEvaluatorAgent()
                print("‚úÖ Evaluator agent agregado")
            
            orchestrator = MedicalAgentsOrchestrator(agents)
            print("‚úÖ LangGraph Orchestrator inicializado con agentes reales")
            return orchestrator
        except Exception as e:
            st.error(f"Error inicializando LangGraph Orchestrator: {e}")
            print(f"‚ùå Error en LangGraph: {e}")
    
    print("‚ö†Ô∏è Usando orquestador mock")
    return create_mock_orchestrator()

def create_mock_orchestrator():
    """Crea un orquestador mock para desarrollo"""
    mock_agents = {
        "coordinator": MockAgent("Coordinador"),
        "analyzer": MockAgent("Analista Cl√≠nico"),
        "generator": MockAgent("Generador Sint√©tico"),
        "simulator": MockAgent("Simulador de Pacientes"),
        "evaluator": MockAgent("Evaluador de Utilidad")
    }
    
    # Agregar validador real si est√° disponible
    if VALIDATOR_AVAILABLE:
        try:
            mock_agents["validator"] = MedicalValidatorAgent()
            print("‚úÖ Validator real agregado al orquestador mock")
        except Exception as e:
            mock_agents["validator"] = MockAgent("Validador M√©dico")
            print(f"‚ö†Ô∏è Error agregando validator real al mock, usando mock: {e}")
    else:
        mock_agents["validator"] = MockAgent("Validador M√©dico")
    
    class MockLangGraphOrchestrator:
        def __init__(self, agents):
            self.agents = agents
            self.state = {
                "current_agent": "coordinator",
                "dataset_uploaded": False,
                "analysis_complete": False,
                "generation_complete": False,
                "validation_complete": False,
                "synthetic_data": None,
                "context": {}
            }
        
        async def process_user_input(self, user_input: str, context: dict = None):
            """Procesa input del usuario (versi√≥n mock)"""
            self.state["context"] = context or {}
            
            # Detectar intenci√≥n y ejecutar agente correspondiente
            if any(word in user_input.lower() for word in ["analizar", "an√°lisis", "analiza"]):
                self.state["current_agent"] = "analyzer"
                agent = self.agents["analyzer"]
                response = await agent.process(user_input, context)
                return response
            elif any(word in user_input.lower() for word in ["generar", "sint√©tico", "sint√©ticos", "genera"]):
                self.state["current_agent"] = "generator"
                
                # Detectar modelo espec√≠fico en lenguaje natural
                model_type = "ctgan"  # Default
                if any(word in user_input.lower() for word in ["tvae", "variational", "autoencoder"]):
                    model_type = "tvae"
                elif any(word in user_input.lower() for word in ["sdv", "vault", "synthetic data vault"]):
                    model_type = "sdv"
                elif any(word in user_input.lower() for word in ["ctgan", "gan", "generative adversarial"]):
                    model_type = "ctgan"
                
                # Detectar n√∫mero de muestras
                import re
                numbers = re.findall(r'\b(\d+)\b', user_input)
                num_samples = int(numbers[0]) if numbers else 100
                
                # Configurar contexto con par√°metros
                context = context or {}
                context['parameters'] = {
                    'model_type': model_type,
                    'num_samples': num_samples
                }
                
                agent = self.agents["generator"]
                response = await agent.process(user_input, context)
                return response
            elif any(word in user_input.lower() for word in ["validar", "valida", "validaci√≥n"]):
                self.state["current_agent"] = "validator"
                # Usar el validador real si est√° disponible, incluso en modo mock
                if VALIDATOR_AVAILABLE:
                    try:
                        agent = self.agents["validator"]
                        response = await agent.process(user_input, context)
                        return response
                    except Exception as e:
                        return {"message": f"‚ùå Error en validaci√≥n: {str(e)}", "agent": "validator", "error": True}
                else:
                    return {"message": "‚úÖ Validaci√≥n completada (modo simulado)\n\nDatos validados exitosamente.", "agent": "validator"}
            elif any(word in user_input.lower() for word in ["evaluar", "eval√∫a", "calidad"]):
                self.state["current_agent"] = "evaluator"
                return {"message": "üìä Evaluaci√≥n completada (modo simulado)\n\nCalidad de datos: Excelente", "agent": "evaluator"}
            elif any(word in user_input.lower() for word in ["simular", "simula", "paciente"]):
                self.state["current_agent"] = "simulator"
                return {"message": "üè• Simulaci√≥n completada (modo simulado)\n\nEvoluci√≥n de pacientes simulada.", "agent": "simulator"}
            else:
                # Para preguntas conversacionales, m√©dicas o generales
                self.state["current_agent"] = "coordinator"
                
                # Si Azure est√° configurado, usar el coordinador real
                if AZURE_CONFIGURED and connection_test:
                    agent = self.agents["coordinator"]
                    response = await agent.process(user_input, context)
                    return response
                else:
                    # Respuesta mock inteligente para preguntas m√©dicas
                    return self._generate_mock_medical_response(user_input, context)
        
        def _generate_mock_medical_response(self, user_input: str, context: dict = None):
            """Genera respuestas mock inteligentes para preguntas m√©dicas"""
            user_lower = user_input.lower()
            
            # Detectar diferentes tipos de preguntas m√©dicas
            if any(term in user_lower for term in ["diabetes", "glucosa", "insulina"]):
                return {
                    "message": "ü©∫ **Informaci√≥n sobre Diabetes**\n\nLa diabetes es una enfermedad cr√≥nica que afecta la forma en que el cuerpo procesa la glucosa. Los principales factores de riesgo incluyen:\n\n‚Ä¢ **Tipo 1**: Factores gen√©ticos e inmunol√≥gicos\n‚Ä¢ **Tipo 2**: Sobrepeso, sedentarismo, historia familiar\n‚Ä¢ **Gestacional**: Cambios hormonales durante el embarazo\n\n**Recomendaci√≥n**: Para an√°lisis detallado de datos diab√©ticos, sube un dataset y solicita un an√°lisis espec√≠fico.\n\n*Nota: Esta es informaci√≥n general. Consulta siempre con un profesional m√©dico.*",
                    "agent": "coordinator",
                    "topic": "diabetes"
                }
            elif any(term in user_lower for term in ["covid", "coronavirus", "sars-cov"]):
                return {
                    "message": "ü¶† **Informaci√≥n sobre COVID-19**\n\nFactores de riesgo identificados en datos cl√≠nicos:\n\n‚Ä¢ **Edad**: Pacientes > 65 a√±os\n‚Ä¢ **Comorbilidades**: Diabetes, hipertensi√≥n, EPOC\n‚Ä¢ **Estado inmunol√≥gico**: Inmunosupresi√≥n\n‚Ä¢ **Factores cardiovasculares**: Enfermedad card√≠aca previa\n\n**Nuestro sistema** puede analizar datasets COVID-19 y generar datos sint√©ticos que preserven estos patrones epidemiol√≥gicos.\n\n*Datos basados en estudios internacionales publicados.*",
                    "agent": "coordinator",
                    "topic": "covid19"
                }
            elif any(term in user_lower for term in ["hipertensi√≥n", "presi√≥n", "cardiovascular"]):
                return {
                    "message": "‚ù§Ô∏è **Factores de Riesgo Cardiovascular**\n\nPrincipales factores identificados en estudios cl√≠nicos:\n\n‚Ä¢ **Modificables**: Tabaquismo, colesterol alto, sedentarismo\n‚Ä¢ **No modificables**: Edad, sexo, historia familiar\n‚Ä¢ **Metab√≥licos**: Diabetes, obesidad, s√≠ndrome metab√≥lico\n‚Ä¢ **Otros**: Estr√©s, apnea del sue√±o, enfermedad renal\n\n**¬øTienes datos cardiovasculares?** Puedo ayudarte a analizarlos y generar datasets sint√©ticos para investigaci√≥n.\n\n*Informaci√≥n basada en gu√≠as cl√≠nicas internacionales.*",
                    "agent": "coordinator",
                    "topic": "cardiovascular"
                }
            elif any(term in user_lower for term in ["c√°ncer", "oncolog√≠a", "tumor", "met√°stasis"]):
                return {
                    "message": "üéóÔ∏è **Informaci√≥n Oncol√≥gica**\n\nFactores relevantes en an√°lisis de datos oncol√≥gicos:\n\n‚Ä¢ **Estadificaci√≥n**: TNM, grado histol√≥gico\n‚Ä¢ **Biomarcadores**: Receptores hormonales, HER2, mutaciones\n‚Ä¢ **Tratamiento**: Quimioterapia, radioterapia, inmunoterapia\n‚Ä¢ **Seguimiento**: Supervivencia libre de enfermedad, calidad de vida\n\n**Capacidades del sistema**: An√°lisis de cohortes oncol√≥gicas y generaci√≥n de datos sint√©ticos preservando caracter√≠sticas pron√≥sticas.\n\n*Para an√°lisis espec√≠ficos, considera subir datos anonimizados.*",
                    "agent": "coordinator",
                    "topic": "oncology"
                }
            elif any(term in user_lower for term in ["hola", "saludo", "buenos d√≠as", "buenas tardes", "como estas"]):
                has_dataset = context and context.get("dataset_uploaded", False)
                dataset_msg = ""
                if has_dataset:
                    filename = context.get("filename", "archivo")
                    rows = context.get("rows", 0)
                    cols = context.get("columns", 0)
                    dataset_msg = f"\n\nüìä **Dataset actual**: {filename} ({rows:,} filas, {cols} columnas)"
                
                return {
                    "message": f"üëã **¬°Hola!** Estoy muy bien, gracias por preguntar.\n\nSoy tu asistente de IA especializado en datos cl√≠nicos sint√©ticos.\n\nüî¨ **Estado del sistema**: {'‚úÖ Azure OpenAI Conectado' if connection_test else 'üîÑ Modo Simulado'}{dataset_msg}\n\n**¬øEn qu√© puedo ayudarte?**\n‚Ä¢ Analizar datasets m√©dicos\n‚Ä¢ Generar datos sint√©ticos seguros\n‚Ä¢ Responder preguntas sobre medicina\n‚Ä¢ Validar coherencia cl√≠nica\n\n¬°Preg√∫ntame cualquier cosa sobre medicina o datos cl√≠nicos!",
                    "agent": "coordinator",
                    "topic": "greeting"
                }
            elif any(term in user_lower for term in ["ayuda", "help", "qu√© puedes hacer"]):
                return {
                    "message": "üìã **Gu√≠a de Uso - Patient IA**\n\n**ü§ñ Comandos principales:**\n‚Ä¢ `Analiza estos datos` - Explora patrones en tu dataset\n‚Ä¢ `Genera 1000 muestras con CTGAN` - Crea datos sint√©ticos\n‚Ä¢ `Valida la coherencia m√©dica` - Verifica calidad cl√≠nica\n\n**ü©∫ Consultas m√©dicas:**\n‚Ä¢ Factores de riesgo cardiovascular\n‚Ä¢ Informaci√≥n sobre diabetes, COVID-19\n‚Ä¢ An√°lisis epidemiol√≥gico\n‚Ä¢ Interpretaci√≥n de biomarcadores\n\n**üìä Tipos de datos soportados:**\n‚Ä¢ CSV, Excel (.xlsx, .xls)\n‚Ä¢ Historiales cl√≠nicos\n‚Ä¢ Datos de laboratorio\n‚Ä¢ Registros epidemiol√≥gicos\n\n¬øHay algo espec√≠fico en lo que te pueda ayudar?",
                    "agent": "coordinator",
                    "topic": "help"
                }
            else:
                # Respuesta general para otras preguntas m√©dicas
                return {
                    "message": f"ü§î **Respuesta m√©dica (modo simulado)**\n\nHe recibido tu consulta: *\"{user_input}\"*\n\nüìö Como asistente de IA m√©dica, puedo ayudarte con:\n‚Ä¢ An√°lisis de datasets cl√≠nicos\n‚Ä¢ Informaci√≥n sobre enfermedades comunes\n‚Ä¢ Interpretaci√≥n de factores de riesgo\n‚Ä¢ Generaci√≥n de datos sint√©ticos\n\n**Para respuestas m√°s precisas**, configura Azure OpenAI o formula tu pregunta de manera m√°s espec√≠fica.\n\n*Recuerda: Esta informaci√≥n es para fines educativos. Consulta siempre con profesionales m√©dicos.*",
                    "agent": "coordinator",
                    "topic": "general_medical"
                }
    
    return MockLangGraphOrchestrator(mock_agents)

@st.cache_resource
def initialize_orchestrator():
    """Inicializa el orquestador principal"""
    return initialize_langgraph_orchestrator()

# Inicializaci√≥n del estado
if 'orchestrator' not in st.session_state:
    st.session_state.orchestrator = initialize_orchestrator()

if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []

if 'context' not in st.session_state:
    st.session_state.context = {}

if 'file_uploaded' not in st.session_state:
    st.session_state.file_uploaded = False

if 'config' not in st.session_state:
    st.session_state.config = {
        'max_synthetic_rows': 500,
        'analysis_mode': 'B√°sico',
        'enable_validation': True
    }

def limit_chat_history():
    """Limita el historial de chat para evitar exceso de tokens"""
    if len(st.session_state.chat_history) > 20:
        st.session_state.chat_history = st.session_state.chat_history[-20:]

def process_uploaded_file(uploaded_file=None):
    """Procesa un archivo cargado desde el sidebar"""
    if uploaded_file:
        try:
            with st.spinner(f"Procesando {uploaded_file.name}..."):
                if uploaded_file.name.endswith('.csv'):
                    df = pd.read_csv(uploaded_file)
                else:
                    df = pd.read_excel(uploaded_file)
                
                st.session_state.context.update({
                    'dataframe': df,
                    'dataset_uploaded': True,
                    'filename': uploaded_file.name,
                    'rows': df.shape[0],
                    'columns': df.shape[1]
                })
                
                st.session_state.file_uploaded = True
                st.session_state.uploaded_file = uploaded_file
                
                st.success(f"‚úÖ Archivo {uploaded_file.name} cargado exitosamente")
                
                st.session_state.chat_history.append({
                    "role": "assistant",
                    "content": f"üìä **Archivo cargado exitosamente**\n\n**{uploaded_file.name}**\n- {df.shape[0]:,} filas\n- {df.shape[1]} columnas\n\n¬øQu√© te gustar√≠a hacer con estos datos?",
                    "dataset_loaded": True,
                    "dataset_info": {
                        "filename": uploaded_file.name,
                        "rows": df.shape[0],
                        "columns": df.shape[1],
                        "dtypes": df.dtypes.value_counts().to_dict()
                    },
                    "dataset_preview": df.head().to_dict('records')
                })
                
                return True
                
        except Exception as e:
            st.error(f"Error al procesar el archivo: {e}")
            return False
    return False

def handle_synthetic_data_response(response, context=None):
    """Maneja la respuesta de generaci√≥n sint√©tica de forma centralizada"""
    if "synthetic_data" in response:
        synthetic_df = response["synthetic_data"]
        generation_info = response.get("generation_info", {})
        
        # Si generation_info est√° vac√≠o o incompleto, crear uno por defecto
        if not generation_info or not generation_info.get('model_type'):
            # Intentar extraer informaci√≥n del contexto o response
            context = context or {}
            parameters = context.get("parameters", {})
            
            # Crear generation_info por defecto con informaci√≥n disponible
            generation_info = {
                "model_type": parameters.get("model_type", "ctgan"),  # Modelo por defecto
                "num_samples": len(synthetic_df),
                "columns_used": len(synthetic_df.columns),
                "selection_method": "Columnas seleccionadas" if context.get('selected_columns') else "Autom√°tico",
                "timestamp": datetime.now().strftime('%Y%m%d_%H%M%S')
            }
        else:
            # Asegurar que tiene timestamp
            if 'timestamp' not in generation_info:
                generation_info["timestamp"] = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # Asegurar que num_samples coincide con los datos reales
            if 'num_samples' not in generation_info or generation_info['num_samples'] != len(synthetic_df):
                generation_info["num_samples"] = len(synthetic_df)
            
            # Asegurar que columns_used coincide con los datos reales
            if 'columns_used' not in generation_info or generation_info['columns_used'] != len(synthetic_df.columns):
                generation_info["columns_used"] = len(synthetic_df.columns)
        
        # Guardar en session_state
        st.session_state.context["synthetic_data"] = synthetic_df
        st.session_state.context["generation_info"] = generation_info
        
        return True
    return False

# Header principal con logo integrado
logo_path = os.path.join(project_root, "assets", "logo_patientia.png")
if os.path.exists(logo_path):
    col_left, col_logo, col_title, col_right = st.columns([5.7, 0.7, 6, 2])
    with col_logo:
        st.image(logo_path, width=54)
    with col_title:
        st.markdown(
            "<h1 style='font-size: 2.2rem; font-weight: 600; color: #1f2937; margin: 0; display: flex; align-items: center; height: 54px;'>Patient-IA</h1>",
            unsafe_allow_html=True
        )
else:
    st.markdown("""
    <div class="main-header">
        <h1>Patient-IA</h1>
    </div>
    """, unsafe_allow_html=True)

# Status indicator actualizado
if AZURE_CONFIGURED and connection_test:
    status_text = "‚úÖ Azure OpenAI Conectado"
    status_color = "#10b981"
elif AZURE_CONFIGURED:
    status_text = "üü° Azure Configurado (Sin conexi√≥n)"
    status_color = "#f59e0b"
else:
    status_text = "üîÑ Modo Simulado"
    status_color = "#6b7280"

st.markdown(f"""
<div class="status-indicator" style="border-color: {status_color}20; background: {status_color}10; color: {status_color};">
    <span>{status_text}</span>
    <span style="margin-left: 10px;">‚Ä¢</span>
    <span>{len(st.session_state.chat_history)} mensajes</span>
    {"<span style='margin-left: 10px;'>‚Ä¢ Archivo cargado</span>" if st.session_state.get('file_uploaded', False) else ""}
</div>
""", unsafe_allow_html=True)

# Sidebar mejorado con funcionalidades √∫tiles
with st.sidebar:
    st.header("Panel de Control")
    
    # Cargar Dataset
    st.subheader("Cargar Dataset")
    uploaded_file_sidebar = st.file_uploader(
        "Sube un archivo CSV o Excel", 
        type=["csv", "xlsx", "xls"], 
        key="sidebar_uploader"
    )
    
    # Procesar archivo si se ha subido uno nuevo
    if uploaded_file_sidebar is not None:
        current_file_key = f"{uploaded_file_sidebar.name}_{uploaded_file_sidebar.size}"
        previous_file_key = st.session_state.get('last_processed_file_key', '')
        
        if current_file_key != previous_file_key:
            st.session_state.last_processed_file_key = current_file_key
            if process_uploaded_file(uploaded_file_sidebar):
                st.rerun()
    
    # Informaci√≥n del dataset si est√° cargado
    if st.session_state.get('file_uploaded', False):
        st.subheader("Dataset Actual")
        filename = st.session_state.context.get("filename", "archivo")
        rows = st.session_state.context.get("rows", 0)
        cols = st.session_state.context.get("columns", 0)
        
        st.success(f"‚úÖ **{filename}**")
        st.info(f"üìà **{rows:,}** filas, **{cols}** columnas")
        
        # Bot√≥n para seleccionar columnas para generaci√≥n sint√©tica
        if COLUMN_SELECTOR_AVAILABLE and 'dataframe' in st.session_state.context:
            if st.button("üîç Seleccionar Columnas", use_container_width=True):
                st.session_state.show_column_selector = True
        
        # Bot√≥n para quitar archivo
        if st.button("Quitar Archivo", use_container_width=True):
            keys_to_reset = ['file_uploaded', 'uploaded_file', 'context', 'analysis_complete', 'show_column_selector', 'selected_columns']
            for key in keys_to_reset:
                if key in st.session_state:
                    del st.session_state[key]
            st.rerun()
        
        # Bot√≥n para nueva conversaci√≥n
        if st.button("Nueva Conversaci√≥n", use_container_width=True):
            for key in list(st.session_state.keys()):
                if key != 'orchestrator':
                    del st.session_state[key]
            st.rerun()
    
    # Comandos r√°pidos
    st.subheader("Comandos R√°pidos")
    if st.button("Analizar Datos", use_container_width=True):
        if st.session_state.get('file_uploaded'):
            st.session_state.quick_command = "analizar datos"
        else:
            st.warning("Primero carga un archivo")
    
    if st.button("ü§ñ Generar Sint√©ticos", use_container_width=True):
        if st.session_state.get('file_uploaded'):
            # Mostrar selector de modelos en lugar de quick_command
            st.session_state.show_model_selector = True
            st.session_state.selected_model = 'ctgan'  # Modelo por defecto
        else:
            st.warning("Primero carga un archivo")
    
    if st.button("Validar Datos", use_container_width=True):
        if st.session_state.get('file_uploaded'):
            st.session_state.quick_command = "validar datos"
        else:
            st.warning("Primero carga un archivo")
    
    if st.button("Evaluar Calidad", use_container_width=True):
        if st.session_state.get('file_uploaded'):
            st.session_state.quick_command = "evaluar calidad"
        else:
            st.warning("Primero carga un archivo")
    
    if st.button("Simular Paciente", use_container_width=True):
        if st.session_state.get('file_uploaded'):
            st.session_state.quick_command = "simular paciente"
        else:
            st.warning("Primero carga un archivo")
    
    # Configuraci√≥n avanzada
    with st.expander("Configuraci√≥n Avanzada"):
        st.markdown("**L√≠mites de Generaci√≥n:**")
        max_synthetic_rows = st.slider("M√°x. registros sint√©ticos", 50, 1000, 500)
        
        st.markdown("**Modo de An√°lisis:**")
        analysis_mode = st.selectbox("Tipo de an√°lisis", ["B√°sico", "Detallado", "Experto"])
        
        st.markdown("**Validaci√≥n:**")
        enable_validation = st.checkbox("Validaci√≥n autom√°tica", value=True)
        
        st.session_state.config = {
            'max_synthetic_rows': max_synthetic_rows,
            'analysis_mode': analysis_mode,
            'enable_validation': enable_validation
        }
    
    # Selector de modelo para generaci√≥n sint√©tica
    if st.session_state.get('show_model_selector', False):
        st.markdown("---")
        st.subheader("ü§ñ Selecci√≥n de Modelo de Generaci√≥n")
        
        # Explicaciones de modelos
        model_info = {
            'ctgan': {
                'name': 'CTGAN (Conditional Tabular GAN)',
                'description': 'Red neuronal generativa adversarial especializada en datos tabulares.',
                'pros': '‚Ä¢ Excelente para datos mixtos (categ√≥ricos + num√©ricos)\n‚Ä¢ Maneja correlaciones complejas\n‚Ä¢ R√°pido entrenamiento',
                'cons': '‚Ä¢ Puede generar outliers\n‚Ä¢ Requiere ajuste de hiperpar√°metros',
                'best_for': 'Datasets m√©dicos con variables categ√≥ricas y num√©ricas mezcladas',
                'color': 'blue'
            },
            'tvae': {
                'name': 'TVAE (Tabular Variational AutoEncoder)', 
                'description': 'Autoencoder variacional optimizado para datos tabulares.',
                'pros': '‚Ä¢ Preserva distribuciones estad√≠sticas\n‚Ä¢ Menos propenso a outliers\n‚Ä¢ Estable y confiable',
                'cons': '‚Ä¢ Puede ser conservador\n‚Ä¢ Menor diversidad en algunos casos',
                'best_for': 'Cuando se requiere alta fidelidad estad√≠stica',
                'color': 'green'
            },
            'sdv': {
                'name': 'SDV (Synthetic Data Vault)',
                'description': 'Suite completa de s√≠ntesis con m√∫ltiples algoritmos.',
                'pros': '‚Ä¢ Algoritmos m√∫ltiples integrados\n‚Ä¢ Optimizado para datos m√©dicos\n‚Ä¢ Validaci√≥n autom√°tica',
                'cons': '‚Ä¢ Mayor complejidad computacional\n‚Ä¢ Tiempo de entrenamiento m√°s largo',
                'best_for': 'Proyectos que requieren m√°xima calidad y validaci√≥n',
                'color': 'orange'
            }
        }
        
        # Selector de modelo por defecto
        default_model = st.session_state.get('selected_model', 'ctgan')
        
        # Crear tabs para cada modelo
        tab1, tab2, tab3 = st.tabs(['üîµ CTGAN', 'üü¢ TVAE', 'üü† SDV'])
        
        with tab1:
            info = model_info['ctgan']
            st.markdown(f"**{info['name']}**")
            st.write(info['description'])
            
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("**‚úÖ Ventajas:**")
                st.markdown(info['pros'])
            with col2:
                st.markdown("**‚ö†Ô∏è Consideraciones:**")
                st.markdown(info['cons'])
            
            st.info(f"**üéØ Ideal para:** {info['best_for']}")
            
            if st.button("Seleccionar CTGAN", key="select_ctgan", use_container_width=True):
                st.session_state.selected_model = 'ctgan'
                st.success("‚úÖ CTGAN seleccionado")
        
        with tab2:
            info = model_info['tvae']
            st.markdown(f"**{info['name']}**")
            st.write(info['description'])
            
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("**‚úÖ Ventajas:**")
                st.markdown(info['pros'])
            with col2:
                st.markdown("**‚ö†Ô∏è Consideraciones:**")
                st.markdown(info['cons'])
            
            st.info(f"**üéØ Ideal para:** {info['best_for']}")
            
            if st.button("Seleccionar TVAE", key="select_tvae", use_container_width=True):
                st.session_state.selected_model = 'tvae'
                st.success("‚úÖ TVAE seleccionado")
        
        with tab3:
            info = model_info['sdv']
            st.markdown(f"**{info['name']}**")
            st.write(info['description'])
            
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("**‚úÖ Ventajas:**")
                st.markdown(info['pros'])
            with col2:
                st.markdown("**‚ö†Ô∏è Consideraciones:**")
                st.markdown(info['cons'])
            
            st.info(f"**üéØ Ideal para:** {info['best_for']}")
            
            if st.button("Seleccionar SDV", key="select_sdv", use_container_width=True):
                st.session_state.selected_model = 'sdv'
                st.success("‚úÖ SDV seleccionado")
        
        st.markdown("---")
        
        # Configuraci√≥n adicional
        col1, col2 = st.columns(2)
        with col1:
            num_samples = st.number_input("N√∫mero de registros a generar", min_value=10, max_value=1000, value=100, step=10)
        with col2:
            st.markdown("**Modelo seleccionado:**")
            selected_model = st.session_state.get('selected_model', 'ctgan')
            st.markdown(f"ü§ñ **{model_info[selected_model]['name']}**")
        
        # Botones de acci√≥n
        col1, col2, col3 = st.columns([1, 1, 1])
        
        with col1:
            if st.button("üöÄ Generar Datos", use_container_width=True):
                # Preparar contexto para generaci√≥n
                context_for_generation = st.session_state.context.copy()
                context_for_generation['parameters'] = {
                    'model_type': st.session_state.get('selected_model', 'ctgan'),
                    'num_samples': num_samples
                }
                
                # A√±adir columnas seleccionadas si existen
                if st.session_state.get('selected_columns'):
                    context_for_generation['selected_columns'] = st.session_state.selected_columns
                
                # Simular llamada al generador
                prompt_for_generation = f"Genera {num_samples} registros sint√©ticos usando el modelo {selected_model.upper()}"
                st.session_state.pending_generation = {
                    'prompt': prompt_for_generation,
                    'context': context_for_generation
                }
                st.session_state.show_model_selector = False
                st.success(f"‚úÖ Configuraci√≥n guardada. Generando con {selected_model.upper()}...")
                st.rerun()
        
        with col2:
            if st.button("üîÑ Cambiar Modelo", use_container_width=True):
                st.session_state.selected_model = 'ctgan'  # Reset a default
                st.rerun()
        
        with col3:
            if st.button("‚ùå Cancelar", use_container_width=True):
                st.session_state.show_model_selector = False
                if 'selected_model' in st.session_state:
                    del st.session_state.selected_model
                st.rerun()

    # Informaci√≥n de datos sint√©ticos generados
    if st.session_state.context.get('synthetic_data') is not None:
        st.markdown("---")
        st.subheader("üìä Datos Sint√©ticos Generados")
        
        synthetic_df = st.session_state.context['synthetic_data']
        generation_info = st.session_state.context.get('generation_info', {})
        
        # M√©tricas b√°sicas
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Registros", f"{len(synthetic_df):,}")
        with col2:
            st.metric("Columnas", len(synthetic_df.columns))
        with col3:
            # Mostrar informaci√≥n del modelo de forma m√°s clara
            model_type = generation_info.get('model_type', 'N/A')
            if model_type and model_type != 'N/A':
                model_display = model_type.upper()
            else:
                # Si no tenemos info del modelo, mostrar informaci√≥n del DataFrame
                model_display = "GENERADO"
            st.metric("Modelo", model_display)
        
        # Vista previa de los datos
        st.markdown("**üîç Vista previa de datos sint√©ticos:**")
        st.dataframe(synthetic_df.head(10), use_container_width=True)
        
        # Informaci√≥n de generaci√≥n
        if generation_info:
            with st.expander("üî¨ Detalles de generaci√≥n"):
                col1, col2 = st.columns(2)
                with col1:
                    # Formatear modelo de forma segura
                    model_type = generation_info.get('model_type', 'N/A')
                    if model_type and model_type != 'N/A':
                        st.text(f"Modelo utilizado: {model_type.upper()}")
                    else:
                        st.text(f"Modelo utilizado: Datos sint√©ticos generados")
                    # Formatear n√∫mero de muestras de forma segura
                    num_samples = generation_info.get('num_samples', len(synthetic_df))
                    if isinstance(num_samples, (int, float)):
                        st.text(f"Registros generados: {int(num_samples):,}")
                    else:
                        st.text(f"Registros generados: {len(synthetic_df):,}")
                with col2:
                    selection_method = generation_info.get('selection_method', 'N/A')
                    if selection_method == 'N/A':
                        selection_method = "M√©todo est√°ndar"
                    st.text(f"M√©todo de selecci√≥n: {selection_method}")
                    
                    columns_used = generation_info.get('columns_used', len(synthetic_df.columns))
                    st.text(f"Columnas utilizadas: {columns_used}")
        else:
            # Si no hay generation_info, mostrar informaci√≥n b√°sica del DataFrame
            with st.expander("üî¨ Informaci√≥n de los datos"):
                col1, col2 = st.columns(2)
                with col1:
                    st.text(f"Datos sint√©ticos: Generados exitosamente")
                    st.text(f"Registros: {len(synthetic_df):,}")
                with col2:
                    st.text(f"Columnas: {len(synthetic_df.columns)}")
                    st.text(f"M√©todo: Generaci√≥n est√°ndar")
        
        # Botones de descarga mejorados
        col1, col2, col3 = st.columns([1, 1, 1])
        
        with col1:
            csv_data = synthetic_df.to_csv(index=False)
            timestamp = generation_info.get('timestamp', datetime.now().strftime('%Y%m%d_%H%M%S'))
            filename_csv = f"datos_sinteticos_{generation_info.get('model_type', 'ctgan')}_{len(synthetic_df)}_{timestamp}.csv"
            st.download_button(
                label="üìÑ Descargar CSV",
                data=csv_data,
                file_name=filename_csv,
                mime="text/csv",
                use_container_width=True
            )
        
        with col2:
            json_data = synthetic_df.to_json(orient='records', indent=2)
            filename_json = f"datos_sinteticos_{generation_info.get('model_type', 'ctgan')}_{len(synthetic_df)}_{timestamp}.json"
            st.download_button(
                label="üìã Descargar JSON",
                data=json_data,
                file_name=filename_json,
                mime="application/json",
                use_container_width=True
            )
        
        with col3:
            # Bot√≥n para limpiar datos sint√©ticos
            if st.button("üóëÔ∏è Limpiar", use_container_width=True, help="Limpiar datos sint√©ticos generados"):
                del st.session_state.context['synthetic_data']
                if 'generation_info' in st.session_state.context:
                    del st.session_state.context['generation_info']
                st.success("‚úÖ Datos sint√©ticos eliminados")
                st.rerun()

        
    # Agregar nota de seguridad y privacidad
    st.markdown("""
    ---
    **üîí Privacidad y Seguridad:** Todos los datos se procesan localmente. Los datos sint√©ticos mantienen las propiedades estad√≠sticas 
    del dataset original mientras protegen la identidad de los pacientes individuales.
    """)
    
    # Mostrar estado del sistema en la parte inferior
    st.subheader("üîß Estado del Sistema")
    status_col1, status_col2, status_col3 = st.columns(3)
    with status_col1:
        azure_status = "üü¢ Conectado" if AZURE_CONFIGURED and connection_test else "üü° Modo Simulado"
        st.info(f"**Azure OpenAI:** {azure_status}")
    
    with status_col2:
        agents_status = "üü¢ Disponibles" if AGENTS_AVAILABLE else "üü° Mock Agents"
        st.info(f"**Agentes IA:** {agents_status}")
        
    with status_col3:
        langgraph_status = "üü¢ Activo" if LANGGRAPH_AVAILABLE else "üü° Fallback"
        st.info(f"**LangGraph:** {langgraph_status}")

with st.container():
    # Mostrar historial de chat
    if st.session_state.chat_history:
        for i, message in enumerate(st.session_state.chat_history):
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
                
                # MOSTRAR INFORMACI√ìN ESPECIAL DEL DATASET
                if message.get("dataset_loaded"):
                    dataset_info = message.get("dataset_info", {})
                    dataset_preview = message.get("dataset_preview", [])
                    
                    with st.expander("üëÅÔ∏è Vista previa del dataset", expanded=False):
                        if dataset_preview:
                            preview_df = pd.DataFrame(dataset_preview)
                            st.dataframe(preview_df, use_container_width=True)
                        
                        col1, col2 = st.columns(2)
                        with col1:
                            st.markdown("**üìä Distribuci√≥n de tipos:**")
                            for dtype, count in dataset_info.get('dtypes', {}).items():
                                st.text(f"{dtype}: {count} columnas")
                        
                        with col2:
                            st.markdown("**üìã Informaci√≥n t√©cnica:**")
                            st.text(f"Nombre: {dataset_info.get('filename', 'N/A')}")
                            st.text(f"Filas: {dataset_info.get('rows', 0):,}")
                            st.text(f"Columnas: {dataset_info.get('columns', 0)}")
    else:
        # Interfaz est√°tica de bienvenida (NO es un mensaje de chat)
        st.markdown("""
        <div style="text-align: center; margin-bottom: 2rem;">
            <h1 style="font-size: 2.0rem; font-weight: 600; color: #1f2937; margin-bottom: 0.5rem;">
                Asistente Inteligente para Generaci√≥n de Datos Cl√≠nicos Sint√©ticos
            </h1>
            
        </div>
        """, unsafe_allow_html=True)

        # Crear dos columnas principales para explicar las capacidades del sistema
        col1, col2 = st.columns(2, gap="large")
        
        with col1:
            st.markdown("""
            ### ÔøΩ **Capacidades del Sistema**
            
            **üß† Agentes Especializados:**
            - **Analista Cl√≠nico** - Extrae patrones y estad√≠sticas m√©dicas
            - **Generador Sint√©tico** - Crea datos realistas con CTGAN/TVAE/SDV
            - **Validador M√©dico** - Verifica coherencia cl√≠nica
            - **Simulador de Pacientes** - Modela evoluci√≥n temporal
            - **Evaluador de Utilidad** - Mide calidad y privacidad
            
            **üìä Formatos Soportados:**
            - CSV, Excel (.xlsx, .xls)
            - Datos COVID-19, oncol√≥gicos, cardiol√≥gicos
            - Historiales cl√≠nicos y registros m√©dicos
            """)
        
        with col2:
            st.markdown("""
            ### üöÄ **C√≥mo Empezar**
            
            **1Ô∏è‚É£ Cargar Datos:**
            - Sube tu archivo CSV/Excel con datos cl√≠nicos
            - El sistema detectar√° autom√°ticamente el dominio m√©dico
            - Selecciona las columnas m√°s relevantes (opcional)
            
            **2Ô∏è‚É£ Interactuar:**
            - **"Analiza estos datos"** - Para explorar patrones
            - **"Genera 1000 muestras con CTGAN"** - Para crear sint√©ticos
            - **"Valida la coherencia m√©dica"** - Para verificar calidad
            - **"¬øCu√°les son los factores de riesgo cardiovascular?"** - Consultas m√©dicas
            
            **3Ô∏è‚É£ Descargar:**
            - Descarga los datos sint√©ticos en CSV/JSON
            - Revisa m√©tricas de calidad y privacidad
            """)

# --- L√ìGICA PRINCIPAL DEL CHAT ---
async def main_chat_loop():
    """Funci√≥n as√≠ncrona para manejar el bucle principal del chat."""
    limit_chat_history()
    
    # Procesar comandos r√°pidos del sidebar
    if hasattr(st.session_state, 'quick_command'):
        prompt = st.session_state.quick_command
        del st.session_state.quick_command
        
        st.session_state.chat_history.append({"role": "user", "content": prompt})
        
        with st.spinner("Procesando comando..."):
            # Preparar contexto incluyendo columnas seleccionadas si existen
            context_with_selections = st.session_state.context.copy()
            if st.session_state.get('selected_columns'):
                context_with_selections['selected_columns'] = st.session_state.selected_columns
            
            response = await st.session_state.orchestrator.process_user_input(prompt, context_with_selections)
            full_response = response.get("message", "No se recibi√≥ respuesta.")
            
            st.session_state.chat_history.append({
                "role": "assistant", 
                "content": full_response, 
                "agent": response.get("agent"),
                "dataset_type": response.get("dataset_type")
            })
        
        st.rerun()
        return
    
    # JavaScript para indentaci√≥n autom√°tica
    st.markdown("""
    <script>
        function setupTextIndentation() {
            const textarea = document.querySelector('.stChatInput textarea');
            
            if (textarea) {
                const tenSpaces = '          '; // 10 espacios
                
                textarea.addEventListener('focus', function() {
                    if (this.value === '' || this.value === tenSpaces) {
                        this.value = tenSpaces;
                        this.setSelectionRange(tenSpaces.length, tenSpaces.length);
                    }
                });
                
                textarea.addEventListener('input', function() {
                    if (!this.value.startsWith(tenSpaces)) {
                        const cursorPos = this.selectionStart;
                        const restOfText = this.value.replace(/^\\s*/, '');
                        this.value = tenSpaces + restOfText;
                        this.setSelectionRange(Math.max(tenSpaces.length, cursorPos), Math.max(tenSpaces.length, cursorPos));
                    }
                });
                
                textarea.addEventListener('keydown', function(e) {
                    if (e.key === 'Backspace' && this.selectionStart <= tenSpaces.length) {
                        e.preventDefault();
                    }
                    if (e.key === 'Delete' && this.selectionStart < tenSpaces.length) {
                        e.preventDefault();
                    }
                    if (e.key === 'Home') {
                        e.preventDefault();
                        this.setSelectionRange(tenSpaces.length, tenSpaces.length);
                    }
                });
            }
        }
        
        document.addEventListener('DOMContentLoaded', function() {
            setTimeout(setupTextIndentation, 500);
            setTimeout(setupTextIndentation, 1500);
            setTimeout(setupTextIndentation, 3000);
        });
    </script>
    """, unsafe_allow_html=True)
    
    # Interfaz de selecci√≥n de columnas
    if st.session_state.get('show_column_selector', False) and COLUMN_SELECTOR_AVAILABLE:
        st.markdown("---")
        
        # Crear instancia del selector de columnas
        column_selector = MedicalColumnSelector()
        df = st.session_state.context.get('dataframe')
        
        if df is not None:
            # Validar primero si el dataset es m√©dico v√°lido
            is_valid, validation_errors = column_selector.validate_medical_dataset(df)
            
            if not is_valid:
                st.error("‚ùå **Dataset no v√°lido para generaci√≥n sint√©tica m√©dica**")
                for error in validation_errors:
                    st.warning(f"‚ö†Ô∏è {error}")
                
                if st.button("Cerrar Selector"):
                    st.session_state.show_column_selector = False
                    st.rerun()
            else:
                # Mostrar interfaz de selecci√≥n
                column_selection = column_selector.generate_column_selection_interface(df)
                
                # Botones de acci√≥n
                col1, col2, col3 = st.columns([1, 1, 1])
                
                with col1:
                    if st.button("‚úÖ Confirmar Selecci√≥n", use_container_width=True, disabled=not column_selection.mandatory_fulfilled):
                        st.session_state.selected_columns = column_selection.selected_columns
                        st.session_state.show_column_selector = False
                        st.success(f"Seleccionadas {len(column_selection.selected_columns)} columnas para generaci√≥n sint√©tica")
                        st.rerun()
                
                with col2:
                    if st.button("üîÑ Usar Recomendadas", use_container_width=True):
                        # Usar selecci√≥n autom√°tica recomendada
                        dataset_type = column_selector.detector.detect_dataset_type(df)
                        column_mappings = column_selector.detector.infer_medical_columns(df)
                        recommended = column_selector._get_recommended_columns(dataset_type, column_mappings, df)
                        
                        st.session_state.selected_columns = recommended
                        st.session_state.show_column_selector = False
                        st.success(f"Usando {len(recommended)} columnas recomendadas autom√°ticamente")
                        st.rerun()
                
                with col3:
                    if st.button("‚ùå Cancelar", use_container_width=True):
                        st.session_state.show_column_selector = False
                        st.rerun()
        
        st.markdown("---")
    
    # Mostrar columnas seleccionadas si existen
    if st.session_state.get('selected_columns'):
        columns_display = st.session_state.selected_columns[:5]
        more_text = f" (+{len(st.session_state.selected_columns)-5} m√°s)" if len(st.session_state.selected_columns) > 5 else ""
        
        col1, col2 = st.columns([3, 1])
        with col1:
            st.info(f"üéØ **Columnas seleccionadas:** {', '.join(columns_display)}{more_text}")
        with col2:
            if st.button("‚ùå Limpiar selecci√≥n", help="Eliminar selecci√≥n de columnas"):
                del st.session_state.selected_columns
                st.rerun()
    
    # Procesar generaci√≥n pendiente si existe
    if st.session_state.get('pending_generation'):
        pending = st.session_state.pending_generation
        del st.session_state.pending_generation
        
        # A√±adir el prompt de generaci√≥n al historial
        st.session_state.chat_history.append({"role": "user", "content": pending['prompt']})
        
        with st.spinner("üîÑ Generando datos sint√©ticos..."):
            response = await st.session_state.orchestrator.process_user_input(
                pending['prompt'], 
                pending['context']
            )
            
            # Manejar respuesta de generaci√≥n sint√©tica
            handle_synthetic_data_response(response, pending['context'])
            
            full_response = response.get("message", "No se recibi√≥ respuesta.")
            
            st.session_state.chat_history.append({
                "role": "assistant", 
                "content": full_response, 
                "agent": response.get("agent"),
                "dataset_type": response.get("dataset_type")
            })
        
        st.rerun()
        return

    # Chat input principal de Streamlit
    if prompt := st.chat_input("¬øC√≥mo puedo ayudarte?"):
        st.session_state.chat_history.append({"role": "user", "content": prompt})
        
        # Crear un placeholder para el progreso
        progress_placeholder = st.empty()
        
        with st.spinner("üîÑ Procesando solicitud..."):
            # Preparar contexto incluyendo columnas seleccionadas si existen
            context_with_selections = st.session_state.context.copy()
            if st.session_state.get('selected_columns'):
                context_with_selections['selected_columns'] = st.session_state.selected_columns
                progress_placeholder.info(f"‚úÖ Usando {len(st.session_state.selected_columns)} columnas seleccionadas")
            
            response = await st.session_state.orchestrator.process_user_input(prompt, context_with_selections)
            progress_placeholder.empty()  # Limpiar el mensaje de progreso
            
            # Manejar respuesta de generaci√≥n sint√©tica
            handle_synthetic_data_response(response, context_with_selections)
            
            if response.get("error"):
                error_message = response.get("error")
                if isinstance(error_message, bool):
                    error_message = "Error en el procesamiento del dataset"
                elif not isinstance(error_message, str):
                    error_message = str(error_message)
                
                st.error(f"‚ùå **Error**: {error_message}")
                return
            
            st.session_state.chat_history.append({
                "role": "assistant", 
                "content": response.get("message", "No se recibi√≥ respuesta."), 
                "agent": response.get("agent"),
                "dataset_type": response.get("dataset_type")
            })
        
        st.rerun()

# --- EJECUCI√ìN DEL BUCLE AS√çNCRONO ---
if __name__ == "__main__":
    try:
        asyncio.run(main_chat_loop())
    except Exception as e:
        st.error(f"Ocurri√≥ un error en la aplicaci√≥n: {e}")
