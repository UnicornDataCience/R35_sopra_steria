"""
Fast Medical Orchestrator - Optimizado para respuestas r√°pidas sin timeout
Separa las respuestas conversacionales directas de los workflows de agentes complejos.
"""

import time
import datetime
import os
import re
from typing import Dict, Any, Optional
import asyncio
import logging

# Importar el LLM configurado
try:
    from ..config.llm_config import unified_llm_config
    LLM_AVAILABLE = True
except ImportError:
    LLM_AVAILABLE = False

logger = logging.getLogger(__name__)

class FastMedicalOrchestrator:
    """
    Orquestador optimizado que evita timeouts separando:
    - Respuestas directas del LLM (r√°pidas)
    - Workflows de agentes (para tareas espec√≠ficas)
    """
    
    def __init__(self, agents: Dict[str, Any] = None):
        self.agents = agents or {}
        print(f"üöÄ [{datetime.datetime.now().strftime('%H:%M:%S')}] FastMedicalOrchestrator inicializado")
        
        # Debug de agentes disponibles
        if self.agents:
            agent_names = list(self.agents.keys())
            print(f"üîß [DEBUG] Agentes disponibles en FastOrchestrator: {agent_names}")
        else:
            print(f"‚ö†Ô∏è [DEBUG] No hay agentes disponibles en FastOrchestrator")
        
    def _detect_intention_fast(self, user_input: str, context: dict) -> str:
        """
        Detector r√°pido de intenci√≥n SIN llamar a ning√∫n LLM.
        Clasifica el input en rutas espec√≠ficas.
        """
        user_lower = user_input.lower()
        
        # üéØ COMANDOS ESPEC√çFICOS que requieren agentes
        analyze_keywords = ["analizar", "an√°lisis", "analiza", "explora", "examina", "estudia"]
        if any(word in user_lower for word in analyze_keywords):
            return "analyzer_workflow"
            
        generate_keywords = ["generar", "sint√©tico", "sint√©ticos", "genera", "crear", "ctgan", "tvae", "sdv"]
        if any(word in user_lower for word in generate_keywords):
            return "generator_workflow"
            
        validate_keywords = ["validar", "valida", "validaci√≥n", "verificar", "verifica", "comprobar"]
        if any(word in user_lower for word in validate_keywords):
            return "validator_workflow"
            
        simulate_keywords = ["simular", "simula", "paciente", "evoluci√≥n", "temporal"]
        if any(word in user_lower for word in simulate_keywords):
            return "simulator_workflow"
            
        evaluate_keywords = ["evaluar", "eval√∫a", "calidad", "utilidad", "m√©tricas"]
        if any(word in user_lower for word in evaluate_keywords):
            return "evaluator_workflow"
        
        # üí¨ TODO LO DEM√ÅS va a respuesta directa
        return "direct_llm_response"
    
    def _get_direct_llm_response(self, user_input: str, context: dict = None) -> Dict[str, Any]:
        """
        Respuesta directa del LLM sin pasar por agentes.
        Optimizada para velocidad (2-5 segundos).
        """
        start_time = time.time()
        print(f"‚ö° [{datetime.datetime.now().strftime('%H:%M:%S')}] Respuesta directa LLM para: {user_input[:50]}...")
        
        context = context or {}
        has_dataset = context.get("dataset_uploaded", False)
        
        try:
            # FORZAR GROQ TEMPORALMENTE - DEBUG
            use_groq_force = os.getenv('FORCE_GROQ', 'true').lower() == 'true'
            
            if use_groq_force and os.getenv('GROQ_API_KEY'):
                print(f"üöÄ [DEBUG] Forzando uso de Groq para respuesta directa...")
                from ..config.llm_config import GroqProvider
                
                groq_provider = GroqProvider()
                if groq_provider.available:
                    llm = groq_provider.create_llm(temperature=0.1, max_tokens=200)
                    max_words = 80
                    provider_name = "groq"
                    print(f"‚úÖ [DEBUG] Usando Groq con modelo: {groq_provider.model}")
                else:
                    print(f"‚ùå [DEBUG] Groq no disponible, usando configuraci√≥n unificada")
                    use_groq_force = False
            
            if not use_groq_force and LLM_AVAILABLE and unified_llm_config:
                # Configuraci√≥n optimizada para velocidad seg√∫n el modelo
                current_model = os.getenv('OLLAMA_MODEL', 'llama3.2:1b')
                
                if 'mistral' in current_model.lower():
                    # Configuraci√≥n ultra-r√°pida para Mistral
                    llm = unified_llm_config.create_llm(
                        temperature=0.2,
                        max_tokens=200,  # Respuestas m√°s cortas
                        top_p=0.9,
                        repeat_penalty=1.1
                    )
                    max_words = 50  # Mistral es muy conciso
                elif 'llama3.2:1b' in current_model.lower():
                    # Configuraci√≥n para Llama 3.2 (razonamiento)
                    llm = unified_llm_config.create_llm(
                        temperature=0.1,
                        max_tokens=150,
                        top_p=0.8
                    )
                    max_words = 80
                else:
                    # Configuraci√≥n por defecto
                    llm = unified_llm_config.create_llm(
                        temperature=0.1,
                        max_tokens=1000
                    )
                    max_words = 100
                
                # Prompt optimizado para respuestas r√°pidas y concisas
                system_prompt = f"""Eres un asistente de IA m√©dica. Responde de forma CONCISA y DIRECTA.

REGLAS ESTRICTAS:
- M√°ximo {max_words} palabras por respuesta
- Respuestas directas sin explicaciones largas
- Si es saludo: responde amigablemente
- Si es pregunta m√©dica: informaci√≥n b√°sica + sugerencia de subir dataset
- NO analices datos ni generes contenido (eso requiere comandos espec√≠ficos)

CAPACIDADES DISPONIBLES:
- An√°lisis: "analizar datos"
- Generaci√≥n sint√©tica: "generar sint√©ticos"  
- Validaci√≥n: "validar datos"

Responde SOLO lo esencial."""

                # A√±adir contexto del dataset si existe
                user_prompt = user_input
                if has_dataset:
                    filename = context.get("filename", "dataset")
                    rows = context.get("rows", 0)
                    cols = context.get("columns", 0)
                    user_prompt += f"\n\nCONTEXTO: Tengo cargado el dataset '{filename}' con {rows:,} filas y {cols} columnas."

                # Generar respuesta usando el m√©todo correcto seg√∫n el proveedor
                if unified_llm_config.active_provider == "azure":
                    # Para Azure OpenAI, usar m√©todo chat
                    messages = [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ]
                    response = llm.invoke(messages)
                    content = response.content.strip()
                elif unified_llm_config.active_provider == "ollama":
                    # Para Ollama, usar prompt simple
                    full_prompt = f"{system_prompt}\n\nUsuario: {user_prompt}\nAsistente:"
                    response = llm.invoke(full_prompt)
                    content = response.strip()
                elif unified_llm_config.active_provider == "grok":
                    # Para Grok, usar m√©todo directo
                    full_prompt = f"{system_prompt}\n\nUsuario: {user_prompt}\nAsistente:"
                    content = llm.invoke(full_prompt)
                else:
                    # Fallback gen√©rico
                    full_prompt = f"{system_prompt}\n\nUsuario: {user_prompt}\nAsistente:"
                    content = llm.invoke(full_prompt)
                
                end_time = time.time()
                print(f"‚úÖ [{datetime.datetime.now().strftime('%H:%M:%S')}] LLM respondi√≥ en {end_time - start_time:.2f}s")
                
                # Filtrar etiquetas <think> y otros elementos no deseados
                clean_content = self._clean_llm_response(content)
                
                return {
                    "message": clean_content,
                    "agent": "llm_direct",
                    "response_time": f"{end_time - start_time:.2f}s",
                    "route": "direct"
                }
            else:
                # Fallback a respuestas predefinidas si no hay LLM
                return self._get_fallback_response(user_input, context)
                
        except Exception as e:
            print(f"‚ùå Error en LLM directo: {e}")
            # Fallback a respuestas predefinidas
            return self._get_fallback_response(user_input, context)
    
    def _get_fallback_response(self, user_input: str, context: dict = None) -> Dict[str, Any]:
        """
        Respuestas fallback inteligentes sin LLM.
        Basadas en patrones de reconocimiento.
        """
        user_lower = user_input.lower()
        context = context or {}
        has_dataset = context.get("dataset_uploaded", False)
        
        # Patrones de respuesta m√©dica
        if any(term in user_lower for term in ["diabetes", "glucosa", "insulina"]):
            return {
                "message": "ü©∫ **Informaci√≥n sobre Diabetes**\n\nLa diabetes es una enfermedad cr√≥nica que afecta la forma en que el cuerpo procesa la glucosa. Los principales factores de riesgo incluyen:\n\n‚Ä¢ **Tipo 1**: Factores gen√©ticos e inmunol√≥gicos\n‚Ä¢ **Tipo 2**: Sobrepeso, sedentarismo, historia familiar\n‚Ä¢ **Gestacional**: Cambios hormonales durante el embarazo\n\n**Recomendaci√≥n**: Para an√°lisis detallado de datos diab√©ticos, sube un dataset y solicita un an√°lisis espec√≠fico.\n\n*Nota: Esta es informaci√≥n general. Consulta siempre con un profesional m√©dico.*",
                "agent": "fallback_medical",
                "route": "fallback"
            }
            
        elif any(term in user_lower for term in ["covid", "coronavirus", "sars-cov"]):
            return {
                "message": "ü¶† **Informaci√≥n sobre COVID-19**\n\nFactores de riesgo identificados en datos cl√≠nicos:\n\n‚Ä¢ **Edad**: Pacientes > 65 a√±os\n‚Ä¢ **Comorbilidades**: Diabetes, hipertensi√≥n, EPOC\n‚Ä¢ **Estado inmunol√≥gico**: Inmunosupresi√≥n\n‚Ä¢ **Factores cardiovasculares**: Enfermedad card√≠aca previa\n\n**Nuestro sistema** puede analizar datasets COVID-19 y generar datos sint√©ticos que preserven estos patrones epidemiol√≥gicos.\n\n*Datos basados en estudios internacionales publicados.*",
                "agent": "fallback_medical",
                "route": "fallback"
            }
            
        elif any(term in user_lower for term in ["hipertensi√≥n", "presi√≥n", "cardiovascular"]):
            return {
                "message": "‚ù§Ô∏è **Factores de Riesgo Cardiovascular**\n\nPrincipales factores identificados en estudios cl√≠nicos:\n\n‚Ä¢ **Modificables**: Tabaquismo, colesterol alto, sedentarismo\n‚Ä¢ **No modificables**: Edad, sexo, historia familiar\n‚Ä¢ **Metab√≥licos**: Diabetes, obesidad, s√≠ndrome metab√≥lico\n‚Ä¢ **Otros**: Estr√©s, apnea del sue√±o, enfermedad renal\n\n**¬øTienes datos cardiovasculares?** Puedo ayudarte a analizarlos y generar datasets sint√©ticos para investigaci√≥n.\n\n*Informaci√≥n basada en gu√≠as cl√≠nicas internacionales.*",
                "agent": "fallback_medical",
                "route": "fallback"
            }
            
        elif any(term in user_lower for term in ["hola", "saludo", "buenos d√≠as", "buenas tardes", "como estas"]):
            dataset_msg = ""
            if has_dataset:
                filename = context.get("filename", "archivo")
                rows = context.get("rows", 0)
                dataset_msg = f"\n\nüìä **Dataset activo**: {filename} ({rows:,} registros)"
            
            return {
                "message": f"üëã **¬°Hola!** Estoy muy bien, gracias por preguntar.\n\nSoy tu asistente de IA especializado en datos cl√≠nicos sint√©ticos.{dataset_msg}\n\n**¬øEn qu√© puedo ayudarte?**\n‚Ä¢ Analizar datasets m√©dicos\n‚Ä¢ Generar datos sint√©ticos seguros\n‚Ä¢ Responder preguntas sobre medicina\n‚Ä¢ Validar coherencia cl√≠nica\n\n¬°Preg√∫ntame cualquier cosa sobre medicina o datos cl√≠nicos!",
                "agent": "fallback_greeting",
                "route": "fallback"
            }
            
        elif any(term in user_lower for term in ["ayuda", "help", "qu√© puedes hacer"]):
            return {
                "message": "üìã **Gu√≠a de Uso - Patient IA**\n\n**ü§ñ Comandos principales:**\n‚Ä¢ `Analiza estos datos` - Explora patrones en tu dataset\n‚Ä¢ `Genera 1000 muestras con CTGAN` - Crea datos sint√©ticos\n‚Ä¢ `Valida la coherencia m√©dica` - Verifica calidad cl√≠nica\n\n**ü©∫ Consultas m√©dicas:**\n‚Ä¢ Factores de riesgo cardiovascular\n‚Ä¢ Informaci√≥n sobre diabetes, COVID-19\n‚Ä¢ An√°lisis epidemiol√≥gico\n‚Ä¢ Interpretaci√≥n de biomarcadores\n\n**üìä Tipos de datos soportados:**\n‚Ä¢ CSV, Excel (.xlsx, .xls)\n‚Ä¢ Historiales cl√≠nicos\n‚Ä¢ Datos de laboratorio\n‚Ä¢ Registros epidemiol√≥gicos\n\n¬øHay algo espec√≠fico en lo que te pueda ayudar?",
                "agent": "fallback_help",
                "route": "fallback"
            }
        else:
            # Respuesta general
            return {
                "message": f"ü§î **Consulta recibida**\n\nHe recibido tu consulta: *\"{user_input}\"*\n\nüìö Como asistente de IA m√©dica, puedo ayudarte con:\n‚Ä¢ An√°lisis de datasets cl√≠nicos\n‚Ä¢ Informaci√≥n sobre enfermedades comunes\n‚Ä¢ Interpretaci√≥n de factores de riesgo\n‚Ä¢ Generaci√≥n de datos sint√©ticos\n\n**Para tareas espec√≠ficas**, usa comandos como:\n‚Ä¢ \"Analizar datos\" - Para explorar tu dataset\n‚Ä¢ \"Generar sint√©ticos\" - Para crear datos sint√©ticos\n‚Ä¢ \"Validar datos\" - Para verificar coherencia\n\n*Recuerda: Esta informaci√≥n es para fines educativos. Consulta siempre con profesionales m√©dicos.*",
                "agent": "fallback_general",
                "route": "fallback"
            }
    
    def _clean_llm_response(self, content: str) -> str:
        """
        Limpia la respuesta del LLM eliminando etiquetas de razonamiento y formato no deseado.
        """
        if not content:
            return content
        
        # Eliminar etiquetas <think> completas
        content = re.sub(r'<think>.*?</think>\s*', '', content, flags=re.DOTALL | re.IGNORECASE)
        
        # Eliminar etiquetas <thinking> si existen
        content = re.sub(r'<thinking>.*?</thinking>\s*', '', content, flags=re.DOTALL | re.IGNORECASE)
        
        # Eliminar etiquetas de razonamiento que no se cerraron
        content = re.sub(r'<think>.*$', '', content, flags=re.DOTALL | re.IGNORECASE)
        content = re.sub(r'<thinking>.*$', '', content, flags=re.DOTALL | re.IGNORECASE)
        
        # Limpiar m√∫ltiples espacios y saltos de l√≠nea
        content = re.sub(r'\n\s*\n', '\n\n', content)
        content = re.sub(r'^\s+|\s+$', '', content)
        
        # Si la respuesta est√° vac√≠a despu√©s de limpiar, devolver mensaje por defecto
        if not content.strip():
            content = "Lo siento, no pude generar una respuesta clara. ¬øPuedes reformular tu pregunta?"
        
        return content

    async def _execute_agent_workflow(self, workflow_type: str, user_input: str, context: dict) -> Dict[str, Any]:
        """
        Ejecuta workflows espec√≠ficos de agentes para tareas complejas.
        """
        start_time = time.time()
        print(f"üîß [{datetime.datetime.now().strftime('%H:%M:%S')}] Ejecutando workflow: {workflow_type}")
        
        try:
            if workflow_type == "analyzer_workflow":
                print(f"üîç [DEBUG] Buscando agente 'analyzer' en: {list(self.agents.keys())}")
                if "analyzer" in self.agents:
                    print(f"‚úÖ [DEBUG] Agente analyzer encontrado: {type(self.agents['analyzer'])}")
                    
                    # Verificar si hay dataset en contexto
                    if context and context.get("dataframe") is not None:
                        print(f"üìä [DEBUG] Dataset encontrado para an√°lisis")
                        
                        # Ejecutar an√°lisis universal primero
                        try:
                            from ..adapters.universal_dataset_detector import UniversalDatasetDetector
                            detector = UniversalDatasetDetector()
                            df = context["dataframe"]
                            
                            print(f"üî¨ [DEBUG] Ejecutando an√°lisis universal...")
                            universal_analysis = detector.analyze_dataset(df)
                            
                            # Agregar el an√°lisis universal al contexto
                            context["universal_analysis"] = universal_analysis
                            print(f"‚úÖ [DEBUG] An√°lisis universal completado")
                            
                        except Exception as e:
                            print(f"‚ùå [DEBUG] Error en an√°lisis universal: {e}")
                            return {
                                "message": f"‚ùå **Error en an√°lisis universal**: {str(e)}\n\nNo se pudo completar el an√°lisis autom√°tico del dataset.",
                                "agent": "system",
                                "route": "agent_error"
                            }
                    
                    # Usar el m√©todo process() est√°ndar como los otros agentes
                    response = await self.agents["analyzer"].process(user_input, context)
                    response["route"] = "agent_workflow"
                    return response
                else:
                    print(f"‚ùå [DEBUG] Agente analyzer NO encontrado")
                    return {
                        "message": "üìä **An√°lisis de datos solicitado**\n\nPara realizar un an√°lisis completo, necesito que el agente analizador est√© disponible. Actualmente estoy en modo simplificado.\n\n**Capacidades de an√°lisis:**\n‚Ä¢ Estad√≠sticas descriptivas\n‚Ä¢ Detecci√≥n de patrones m√©dicos\n‚Ä¢ Identificaci√≥n de outliers\n‚Ä¢ Correlaciones cl√≠nicas\n\nPor favor, verifica la configuraci√≥n de los agentes.",
                        "agent": "system",
                        "route": "agent_fallback"
                    }
                    
            elif workflow_type == "generator_workflow":
                if "generator" in self.agents:
                    response = await self.agents["generator"].process(user_input, context)
                    response["route"] = "agent_workflow"
                    return response
                else:
                    return {
                        "message": "üè≠ **Generaci√≥n de datos sint√©ticos solicitada**\n\nPara generar datos sint√©ticos, necesito que el agente generador est√© disponible. Actualmente estoy en modo simplificado.\n\n**Modelos disponibles:**\n‚Ä¢ CTGAN - Para datos mixtos\n‚Ä¢ TVAE - Para preservar distribuciones\n‚Ä¢ SDV - Para m√°xima calidad\n\nPor favor, verifica la configuraci√≥n de los agentes.",
                        "agent": "system",
                        "route": "agent_fallback"
                    }
                    
            elif workflow_type == "validator_workflow":
                if "validator" in self.agents:
                    response = await self.agents["validator"].process(user_input, context)
                    response["route"] = "agent_workflow"
                    return response
                else:
                    return {
                        "message": "üîç **Validaci√≥n de datos solicitada**\n\nPara validar la coherencia cl√≠nica, necesito que el agente validador est√© disponible. Actualmente estoy en modo simplificado.\n\n**Validaciones disponibles:**\n‚Ä¢ Coherencia m√©dica\n‚Ä¢ Rangos de valores\n‚Ä¢ Patrones cl√≠nicos\n‚Ä¢ Calidad de datos\n\nPor favor, verifica la configuraci√≥n de los agentes.",
                        "agent": "system",
                        "route": "agent_fallback"
                    }
            else:
                return {
                    "message": f"‚ùå **Workflow no reconocido**: {workflow_type}\n\nWorkflows disponibles:\n‚Ä¢ analyzer_workflow\n‚Ä¢ generator_workflow\n‚Ä¢ validator_workflow",
                    "agent": "system",
                    "route": "error"
                }
                
        except Exception as e:
            end_time = time.time()
            print(f"‚ùå [{datetime.datetime.now().strftime('%H:%M:%S')}] Error en workflow {workflow_type} despu√©s de {end_time - start_time:.2f}s: {e}")
            return {
                "message": f"‚ùå **Error en {workflow_type}**: {str(e)}\n\nPor favor, intenta de nuevo o usa un comando m√°s simple.",
                "agent": "system",
                "route": "error",
                "error": True
            }
    
    async def process_user_input(self, user_input: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Punto de entrada principal. Decide entre respuesta directa o workflow de agentes.
        """
        start_time = time.time()
        print(f"üéØ [{datetime.datetime.now().strftime('%H:%M:%S')}] FastOrchestrator procesando: {user_input[:50]}...")
        
        context = context or {}
        
        # üîç PASO 1: Detectar intenci√≥n (SIN LLM, muy r√°pido)
        intention = self._detect_intention_fast(user_input, context)
        print(f"üéØ Intenci√≥n detectada: {intention}")
        
        # üöÄ PASO 2: Ejecutar ruta correspondiente
        if intention == "direct_llm_response":
            # RUTA R√ÅPIDA: Respuesta directa del LLM
            response = self._get_direct_llm_response(user_input, context)
        else:
            # RUTA DE AGENTES: Workflow espec√≠fico
            response = await self._execute_agent_workflow(intention, user_input, context)
        
        end_time = time.time()
        total_time = end_time - start_time
        response["total_time"] = f"{total_time:.2f}s"
        
        print(f"‚úÖ [{datetime.datetime.now().strftime('%H:%M:%S')}] FastOrchestrator completado en {total_time:.2f}s")
        return response
    
    def process_user_input_sync(self, user_input: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """Versi√≥n s√≠ncrona que usa el wrapper async-safe"""
        try:
            from ..utils.streamlit_async_wrapper import run_async_safe
            return run_async_safe(self.process_user_input, user_input, context)
        except ImportError:
            # Fallback simple si no hay wrapper
            try:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    return loop.run_until_complete(self.process_user_input(user_input, context))
                finally:
                    loop.close()
            except Exception as e:
                return {
                    "message": f"‚ùå Error ejecutando de forma s√≠ncrona: {str(e)}",
                    "agent": "system",
                    "error": True
                }
