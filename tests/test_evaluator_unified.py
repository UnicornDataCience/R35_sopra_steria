#!/usr/bin/env python3
"""
Test integral del agente evaluador funcionando sin Azure OpenAI
"""

import asyncio
import pandas as pd
import numpy as np
import sys
import os
from pathlib import Path

# AÃ±adir src al path 
project_root = Path(__file__).parent.parent
src_path = project_root / "src"
sys.path.insert(0, str(src_path))

from evaluation.unified_evaluator import UnifiedMedicalEvaluator

def create_sample_covid_data():
    """Crea un dataset de muestra con datos COVID-19 sintÃ©ticos para testing"""
    np.random.seed(42)
    
    data = {
        'PATIENT ID': [f'P{i:04d}' for i in range(1, 101)],
        'EDAD/AGE': np.random.randint(18, 90, 100),
        'SEXO/SEX': np.random.choice(['M', 'F'], 100),
        'UCI_DIAS/ICU_DAYS': np.random.randint(0, 15, 100),
        'TEMP_ING/INPAT': np.random.normal(37.5, 1.5, 100).round(1),
        'SAT_02_ING/INPAT': np.random.randint(75, 99, 100),
        'RESULTADO/VAL_RESULT': np.random.choice([0, 1], 100),
        'FARMACO/DRUG_NOMBRE_COMERCIAL/COMERCIAL_NAME': np.random.choice([
            'DEXAMETASONA', 'AZITROMICINA', 'ENOXAPARINA', 'FUROSEMIDA',
            'REMDESIVIR', 'PARACETAMOL', 'OMEPRAZOL'
        ], 100),
        'DIAG ING/INPAT': np.random.choice([
            'NEUMONIA COVID-19', 'INSUFICIENCIA RESPIRATORIA', 
            'SEPSIS', 'SHOCK SEPTICO', 'FALLO MULTIORGANICO'
        ], 100)
    }
    
    return pd.DataFrame(data)

def test_unified_evaluator():
    """Test del evaluador unificado sin dependencias de Azure"""
    
    print("ğŸ§ª === TEST DEL EVALUADOR UNIFICADO ===")
    print()
    
    # 1. Crear datos de muestra
    print("ğŸ“Š 1. Creando datasets de muestra...")
    original_data = create_sample_covid_data()
    
    # Crear datos sintÃ©ticos simulados (ligeramente modificados)
    synthetic_data = original_data.copy()
    
    # Aplicar modificaciones solo a columnas numÃ©ricas
    numeric_cols = synthetic_data.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if col == 'EDAD/AGE':
            synthetic_data[col] = synthetic_data[col] + np.random.randint(-5, 5, len(synthetic_data))
            synthetic_data[col] = synthetic_data[col].clip(lower=0, upper=120)  # Edades vÃ¡lidas
        elif col == 'TEMP_ING/INPAT':
            synthetic_data[col] = synthetic_data[col] + np.random.normal(0, 0.3, len(synthetic_data))
            synthetic_data[col] = synthetic_data[col].clip(lower=35.0, upper=42.0)  # Temp vÃ¡lidas
        elif col == 'UCI_DIAS/ICU_DAYS':
            synthetic_data[col] = synthetic_data[col] + np.random.randint(-2, 2, len(synthetic_data))
            synthetic_data[col] = synthetic_data[col].clip(lower=0, upper=30)  # DÃ­as vÃ¡lidos
        elif col == 'SAT_02_ING/INPAT':
            synthetic_data[col] = synthetic_data[col] + np.random.randint(-5, 5, len(synthetic_data))
            synthetic_data[col] = synthetic_data[col].clip(lower=70, upper=100)  # SaturaciÃ³n vÃ¡lida
    
    print(f"   âœ… Dataset original: {len(original_data)} filas, {len(original_data.columns)} columnas")
    print(f"   âœ… Dataset sintÃ©tico: {len(synthetic_data)} filas, {len(synthetic_data.columns)} columnas")
    print()
    
    # 2. Inicializar evaluador
    print("ğŸ”§ 2. Inicializando evaluador unificado...")
    try:
        evaluator = UnifiedMedicalEvaluator()
        print("   âœ… Evaluador unificado inicializado")
    except Exception as e:
        print(f"   âŒ Error inicializando evaluador: {e}")
        return False
    
    # 3. Ejecutar evaluaciÃ³n completa
    print("ğŸ“Š 3. Ejecutando evaluaciÃ³n completa...")
    try:
        validation_results = {
            "coherence_score": 0.85,
            "overall_quality_score": 0.78,
            "domain": "COVID-19"
        }
        
        domain_info = {
            "dataset_type": "COVID-19",
            "medical_domain": "covid19",
            "detected_features": ["age", "sex", "icu_days", "temperature", "oxygen_saturation"]
        }
        
        # Ejecutar evaluaciÃ³n
        results = evaluator.comprehensive_evaluation(
            original_data, 
            synthetic_data, 
            validation_results
        )
        
        print("   âœ… EvaluaciÃ³n completada exitosamente")
        print()
        
        # 4. Mostrar resultados
        print("ğŸ“ˆ 4. RESULTADOS DE LA EVALUACIÃ“N:")
        final_score = results.get('final_quality_score', 0)
        if isinstance(final_score, (int, float)):
            print(f"   ğŸ¯ Score Final de Calidad: {final_score:.2%}")
        else:
            print(f"   ğŸ¯ Score Final de Calidad: {final_score}")
            
        print(f"   ğŸ“Š Tier de Calidad: {results.get('quality_tier', 'N/A')}")
        
        # Mostrar mÃ©tricas con verificaciÃ³n de tipo
        for metric_name, display_name in [
            ('statistical_fidelity', 'Fidelidad EstadÃ­stica'),
            ('ml_utility_score', 'Utilidad ML'),
            ('medical_coherence', 'Coherencia MÃ©dica'),
            ('privacy_score', 'Score de Privacidad')
        ]:
            value = results.get(metric_name, 0)
            if isinstance(value, (int, float)):
                print(f"   ï¿½ {display_name}: {value:.2%}")
            else:
                print(f"   ğŸ“ˆ {display_name}: {value}")
        
        print()
        print(f"   ğŸ’¡ RecomendaciÃ³n: {results.get('usage_recommendation', 'N/A')}")
        print()
        
        # 5. Generar informe markdown
        print("ğŸ“„ 5. Generando informe markdown...")
        try:
            markdown_report = evaluator.create_markdown_report(results, domain_info)
            print("   âœ… Informe markdown generado")
            print()
            print("   ğŸ“‹ VISTA PREVIA DEL INFORME:")
            print("   " + "="*50)
            # Mostrar las primeras lÃ­neas del informe
            lines = markdown_report.split('\n')[:15]
            for line in lines:
                print(f"   {line}")
            print("   ...")
            print("   " + "="*50)
            print()
        except Exception as e:
            print(f"   âš ï¸ Warning en generaciÃ³n de informe: {e}")
        
        # 6. Verificar mÃ©tricas clave
        print("âœ… 6. VERIFICACIÃ“N DE MÃ‰TRICAS:")
        quality_score = results.get('final_quality_score', 0)
        if quality_score >= 0.7:
            print(f"   ğŸ‰ EXCELENTE: Score de calidad alto ({quality_score:.2%})")
        elif quality_score >= 0.5:
            print(f"   ğŸ‘ BUENO: Score de calidad aceptable ({quality_score:.2%})")
        else:
            print(f"   âš ï¸ REVISAR: Score de calidad bajo ({quality_score:.2%})")
        
        # Verificar que todas las mÃ©tricas principales estÃ©n presentes
        required_metrics = [
            'final_quality_score', 'statistical_fidelity', 'ml_utility_score', 
            'medical_coherence', 'privacy_score', 'usage_recommendation'
        ]
        
        missing_metrics = [m for m in required_metrics if m not in results]
        if not missing_metrics:
            print("   âœ… Todas las mÃ©tricas principales estÃ¡n presentes")
        else:
            print(f"   âš ï¸ MÃ©tricas faltantes: {missing_metrics}")
        
        print()
        return True
        
    except Exception as e:
        print(f"   âŒ Error en evaluaciÃ³n: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_evaluator_components():
    """Test de componentes individuales del evaluador"""
    
    print("ğŸ§ª === TEST DE COMPONENTES INDIVIDUALES ===")
    print()
    
    # Datos de prueba pequeÃ±os
    original_data = create_sample_covid_data().iloc[:20]  # Solo 20 filas
    synthetic_data = original_data.copy()
    
    # Modificar ligeramente
    numeric_cols = synthetic_data.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if len(synthetic_data[col]) > 0:
            synthetic_data[col] = synthetic_data[col] + np.random.normal(0, 0.1, len(synthetic_data))
    
    evaluator = UnifiedMedicalEvaluator()
    
    # Test 1: MÃ©tricas bÃ¡sicas
    print("1. Probando mÃ©tricas bÃ¡sicas de dataset...")
    try:
        basic_metrics = evaluator.evaluate_basic_metrics(original_data, synthetic_data)
        print(f"   âœ… Filas originales: {basic_metrics['original_rows']}")
        print(f"   âœ… Filas sintÃ©ticas: {basic_metrics['synthetic_rows']}")
        print(f"   âœ… Columnas coincidentes: {basic_metrics.get('columns_match', 'N/A')}")
    except Exception as e:
        print(f"   âŒ Error: {e}")
    
    # Test 2: Fidelidad estadÃ­stica
    print("\n2. Probando fidelidad estadÃ­stica...")
    try:
        stat_metrics = evaluator.evaluate_statistical_fidelity(original_data, synthetic_data)
        print(f"   âœ… Fidelidad estadÃ­stica: {stat_metrics.get('statistical_fidelity', 0):.2%}")
        print(f"   âœ… Distribuciones similares: {stat_metrics.get('similar_distributions', 0)}")
    except Exception as e:
        print(f"   âŒ Error: {e}")
    
    # Test 3: Performance ML
    print("\n3. Probando mÃ©tricas de ML...")
    try:
        ml_metrics = evaluator.evaluate_ml_performance(original_data, synthetic_data)
        print(f"   âœ… Score de utilidad ML: {ml_metrics.get('ml_utility_score', 0):.2%}")
    except Exception as e:
        print(f"   âŒ Error: {e}")
    
    print("\nâœ… Tests de componentes individuales completados")
    print()

async def main():
    """FunciÃ³n principal para ejecutar todos los tests"""
    
    print("ğŸš€ INICIANDO TESTS DEL EVALUADOR UNIFICADO")
    print("=" * 60)
    print()
    
    # Test 1: Evaluador unificado completo
    success1 = test_unified_evaluator()
    print()
    print("-" * 60)
    print()
    
    # Test 2: Componentes individuales
    try:
        test_evaluator_components()
        success2 = True
    except Exception as e:
        print(f"âŒ Error en test de componentes: {e}")
        success2 = False
    
    # Resumen final
    print()
    print("=" * 60)
    print("ğŸ RESULTADOS FINALES:")
    print(f"   Test evaluador unificado: {'âœ… PASÃ“' if success1 else 'âŒ FALLÃ“'}")
    print(f"   Test componentes individuales: {'âœ… PASÃ“' if success2 else 'âŒ FALLÃ“'}")
    
    if success1 and success2:
        print()
        print("ğŸ‰ Â¡TODOS LOS TESTS PASARON!")
        print("ğŸ¯ El evaluador unificado funciona correctamente")
        print("ğŸ“Š MÃ©tricas completas de evaluaciÃ³n disponibles")
        print("ğŸ“„ GeneraciÃ³n de informes markdown operativa")
    else:
        print()
        print("âš ï¸ Algunos tests fallaron - revisar configuraciÃ³n")
    
    print()
    print("=" * 60)

if __name__ == "__main__":
    asyncio.run(main())
