#!/usr/bin/env python3
"""
Test integral del agente evaluador funcionando sin Azure OpenAI
"""

import asyncio
import pandas as pd
import numpy as np
import sys
import os
from pathlib import Path

# A√±adir src al path 
project_root = Path(__file__).parent.parent
src_path = project_root / "src"
sys.path.insert(0, str(src_path))

from evaluation.unified_evaluator import UnifiedMedicalEvaluator

def create_sample_covid_data():
    """Crea un dataset de muestra con datos COVID-19 sint√©ticos para testing"""
    np.random.seed(42)
    
    data = {
        'PATIENT ID': [f'P{i:04d}' for i in range(1, 101)],
        'EDAD/AGE': np.random.randint(18, 90, 100),
        'SEXO/SEX': np.random.choice(['M', 'F'], 100),
        'UCI_DIAS/ICU_DAYS': np.random.randint(0, 15, 100),
        'TEMP_ING/INPAT': np.random.normal(37.5, 1.5, 100).round(1),
        'SAT_02_ING/INPAT': np.random.randint(75, 99, 100),
        'RESULTADO/VAL_RESULT': np.random.choice([0, 1], 100),
        'FARMACO/DRUG_NOMBRE_COMERCIAL/COMERCIAL_NAME': np.random.choice([
            'DEXAMETASONA', 'AZITROMICINA', 'ENOXAPARINA', 'FUROSEMIDA',
            'REMDESIVIR', 'PARACETAMOL', 'OMEPRAZOL'
        ], 100),
        'DIAG ING/INPAT': np.random.choice([
            'NEUMONIA COVID-19', 'INSUFICIENCIA RESPIRATORIA', 
            'SEPSIS', 'SHOCK SEPTICO', 'FALLO MULTIORGANICO'
        ], 100)
    }
    
    return pd.DataFrame(data)

def test_unified_evaluator():
    """Test del evaluador unificado sin dependencias de Azure"""
    
    print("üß™ === TEST DEL EVALUADOR UNIFICADO ===")
    print()
    
    # 1. Crear datos de muestra
    print("üìä 1. Creando datasets de muestra...")
    original_data = create_sample_covid_data()
    
    # Crear datos sint√©ticos simulados (ligeramente modificados)
    synthetic_data = original_data.copy()
    
    # Aplicar modificaciones solo a columnas num√©ricas
    numeric_cols = synthetic_data.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if col == 'EDAD/AGE':
            synthetic_data[col] = synthetic_data[col] + np.random.randint(-5, 5, len(synthetic_data))
            synthetic_data[col] = synthetic_data[col].clip(lower=0, upper=120)  # Edades v√°lidas
        elif col == 'TEMP_ING/INPAT':
            synthetic_data[col] = synthetic_data[col] + np.random.normal(0, 0.3, len(synthetic_data))
            synthetic_data[col] = synthetic_data[col].clip(lower=35.0, upper=42.0)  # Temp v√°lidas
        elif col == 'UCI_DIAS/ICU_DAYS':
            synthetic_data[col] = synthetic_data[col] + np.random.randint(-2, 2, len(synthetic_data))
            synthetic_data[col] = synthetic_data[col].clip(lower=0, upper=30)  # D√≠as v√°lidos
        elif col == 'SAT_02_ING/INPAT':
            synthetic_data[col] = synthetic_data[col] + np.random.randint(-5, 5, len(synthetic_data))
            synthetic_data[col] = synthetic_data[col].clip(lower=70, upper=100)  # Saturaci√≥n v√°lida
    
    print(f"   ‚úÖ Dataset original: {len(original_data)} filas, {len(original_data.columns)} columnas")
    print(f"   ‚úÖ Dataset sint√©tico: {len(synthetic_data)} filas, {len(synthetic_data.columns)} columnas")
    print()
    
    # 2. Inicializar evaluador
    print("üîß 2. Inicializando evaluador unificado...")
    try:
        evaluator = UnifiedMedicalEvaluator()
        print("   ‚úÖ Evaluador unificado inicializado")
    except Exception as e:
        print(f"   ‚ùå Error inicializando evaluador: {e}")
        return False
    
    # 3. Ejecutar evaluaci√≥n completa
    print("üìä 3. Ejecutando evaluaci√≥n completa...")
    try:
        validation_results = {
            "coherence_score": 0.85,
            "overall_quality_score": 0.78,
            "domain": "COVID-19"
        }
        
        domain_info = {
            "dataset_type": "COVID-19",
            "medical_domain": "covid19",
            "detected_features": ["age", "sex", "icu_days", "temperature", "oxygen_saturation"]
        }
        
        # Ejecutar evaluaci√≥n
        results = evaluator.comprehensive_evaluation(
            original_data, 
            synthetic_data, 
            validation_results
        )
        
        print("   ‚úÖ Evaluaci√≥n completada exitosamente")
        print()
        
        # 4. Mostrar resultados
        print("üìà 4. RESULTADOS DE LA EVALUACI√ìN:")
        final_score = results.get('final_quality_score', 0)
        if isinstance(final_score, (int, float)):
            print(f"   üéØ Score Final de Calidad: {final_score:.2%}")
        else:
            print(f"   üéØ Score Final de Calidad: {final_score}")
            
        print(f"   üìä Tier de Calidad: {results.get('quality_tier', 'N/A')}")
        
        # Mostrar m√©tricas con verificaci√≥n de tipo
        for metric_name, display_name in [
            ('statistical_fidelity', 'Fidelidad Estad√≠stica'),
            ('ml_utility_score', 'Utilidad ML'),
            ('medical_coherence', 'Coherencia M√©dica'),
            ('privacy_score', 'Score de Privacidad')
        ]:
            value = results.get(metric_name, 0)
            if isinstance(value, (int, float)):
                print(f"   ÔøΩ {display_name}: {value:.2%}")
            else:
                print(f"   üìà {display_name}: {value}")
        
        print()
        print(f"   üí° Recomendaci√≥n: {results.get('usage_recommendation', 'N/A')}")
        print()
        
        # 5. Generar informe markdown
        print("üìÑ 5. Generando informe markdown...")
        try:
            markdown_report = evaluator.create_markdown_report(results, domain_info)
            print("   ‚úÖ Informe markdown generado")
            print()
            print("   üìã VISTA PREVIA DEL INFORME:")
            print("   " + "="*50)
            # Mostrar las primeras l√≠neas del informe
            lines = markdown_report.split('\n')[:15]
            for line in lines:
                print(f"   {line}")
            print("   ...")
            print("   " + "="*50)
            print()
        except Exception as e:
            print(f"   ‚ö†Ô∏è Warning en generaci√≥n de informe: {e}")
        
        # 6. Verificar m√©tricas clave
        print("‚úÖ 6. VERIFICACI√ìN DE M√âTRICAS:")
        quality_score = results.get('final_quality_score', 0)
        if quality_score >= 0.7:
            print(f"   üéâ EXCELENTE: Score de calidad alto ({quality_score:.2%})")
        elif quality_score >= 0.5:
            print(f"   üëç BUENO: Score de calidad aceptable ({quality_score:.2%})")
        else:
            print(f"   ‚ö†Ô∏è REVISAR: Score de calidad bajo ({quality_score:.2%})")
        
        # Verificar que todas las m√©tricas principales est√©n presentes
        required_metrics = [
            'final_quality_score', 'statistical_fidelity', 'ml_utility_score', 
            'medical_coherence', 'privacy_score', 'usage_recommendation'
        ]
        
        missing_metrics = [m for m in required_metrics if m not in results]
        if not missing_metrics:
            print("   ‚úÖ Todas las m√©tricas principales est√°n presentes")
        else:
            print(f"   ‚ö†Ô∏è M√©tricas faltantes: {missing_metrics}")
        
        print()
        return True
        
    except Exception as e:
        print(f"   ‚ùå Error en evaluaci√≥n: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_evaluator_components():
    """Test de componentes individuales del evaluador"""
    
    print("üß™ === TEST DE COMPONENTES INDIVIDUALES ===")
    print()
    
    # Datos de prueba peque√±os
    original_data = create_sample_covid_data().iloc[:20]  # Solo 20 filas
    synthetic_data = original_data.copy()
    
    # Modificar ligeramente
    numeric_cols = synthetic_data.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if len(synthetic_data[col]) > 0:
            synthetic_data[col] = synthetic_data[col] + np.random.normal(0, 0.1, len(synthetic_data))
    
    evaluator = UnifiedMedicalEvaluator()
    
    # Test 1: M√©tricas b√°sicas
    print("1. Probando m√©tricas b√°sicas de dataset...")
    try:
        basic_metrics = evaluator.evaluate_basic_metrics(original_data, synthetic_data)
        print(f"   ‚úÖ Filas originales: {basic_metrics['original_rows']}")
        print(f"   ‚úÖ Filas sint√©ticas: {basic_metrics['synthetic_rows']}")
        print(f"   ‚úÖ Columnas coincidentes: {basic_metrics.get('columns_match', 'N/A')}")
    except Exception as e:
        print(f"   ‚ùå Error: {e}")
    
    # Test 2: Fidelidad estad√≠stica
    print("\n2. Probando fidelidad estad√≠stica...")
    try:
        stat_metrics = evaluator.evaluate_statistical_fidelity(original_data, synthetic_data)
        print(f"   ‚úÖ Fidelidad estad√≠stica: {stat_metrics.get('statistical_fidelity', 0):.2%}")
        print(f"   ‚úÖ Distribuciones similares: {stat_metrics.get('similar_distributions', 0)}")
    except Exception as e:
        print(f"   ‚ùå Error: {e}")
    
    # Test 3: Performance ML
    print("\n3. Probando m√©tricas de ML...")
    try:
        ml_metrics = evaluator.evaluate_ml_performance(original_data, synthetic_data)
        print(f"   ‚úÖ Score de utilidad ML: {ml_metrics.get('ml_utility_score', 0):.2%}")
    except Exception as e:
        print(f"   ‚ùå Error: {e}")
    
    print("\n‚úÖ Tests de componentes individuales completados")
    print()

async def main():
    """Funci√≥n principal para ejecutar todos los tests"""
    
    print("üöÄ INICIANDO TESTS DEL EVALUADOR UNIFICADO")
    print("=" * 60)
    print()
    
    # Test 1: Evaluador unificado completo
    success1 = test_unified_evaluator()
    print()
    print("-" * 60)
    print()
    
    # Test 2: Componentes individuales
    try:
        test_evaluator_components()
        success2 = True
    except Exception as e:
        print(f"‚ùå Error en test de componentes: {e}")
        success2 = False
    
    # Resumen final
    print()
    print("=" * 60)
    print("üèÅ RESULTADOS FINALES:")
    print(f"   Test evaluador unificado: {'‚úÖ PAS√ì' if success1 else '‚ùå FALL√ì'}")
    print(f"   Test componentes individuales: {'‚úÖ PAS√ì' if success2 else '‚ùå FALL√ì'}")
    
    if success1 and success2:
        print()
        print("üéâ ¬°TODOS LOS TESTS PASARON!")
        print("üéØ El evaluador unificado funciona correctamente")
        print("üìä M√©tricas completas de evaluaci√≥n disponibles")
        print("üìÑ Generaci√≥n de informes markdown operativa")
    else:
        print()
        print("‚ö†Ô∏è Algunos tests fallaron - revisar configuraci√≥n")
    
    print()
    print("=" * 60)

if __name__ == "__main__":
    asyncio.run(main())
