#!/usr/bin/env python3
"""
Test espec√≠fico para verificar el workflow del evaluador
"""
import sys
import os
sys.path.append('.')

# Cargar variables de entorno
from dotenv import load_dotenv
load_dotenv()

# FORZAR GROQ
if os.getenv('FORCE_GROQ', 'false').lower() == 'true':
    os.environ['LLM_PROVIDER'] = 'groq'
    print("üöÄ [DEBUG] Variable LLM_PROVIDER forzada a 'groq'")

def test_evaluator_workflow():
    """Test del workflow del evaluador"""
    print("üß™ Testing Evaluator Workflow")
    print("=" * 50)
    
    try:
        # 1. Importar FastOrchestrator
        from src.orchestration.fast_orchestrator import FastMedicalOrchestrator
        print("‚úÖ FastMedicalOrchestrator importado")
        
        # 2. Importar agentes
        from src.agents.coordinator_agent import CoordinatorAgent
        from src.agents.analyzer_agent import ClinicalAnalyzerAgent
        from src.agents.generator_agent import SyntheticGeneratorAgent
        from src.agents.evaluator_agent import UtilityEvaluatorAgent
        
        agents = {
            "coordinator": CoordinatorAgent(),
            "analyzer": ClinicalAnalyzerAgent(),
            "generator": SyntheticGeneratorAgent(),
            "evaluator": UtilityEvaluatorAgent()
        }
        print("‚úÖ Todos los agentes creados")
        
        # 3. Crear orquestador
        orchestrator = FastMedicalOrchestrator(agents)
        print("‚úÖ FastMedicalOrchestrator creado")
        
        # 4. Test detecci√≥n de intenci√≥n
        test_phrases = [
            "evaluar calidad",
            "eval√∫a los datos",
            "m√©tricas de utilidad",
            "calidad de datos sint√©ticos"
        ]
        
        print("\nüîç Testing intention detection:")
        for phrase in test_phrases:
            intention = orchestrator._detect_intention_fast(phrase, {})
            print(f"   '{phrase}' -> {intention}")
            if intention != "evaluator_workflow":
                print(f"‚ùå ERROR: Se esperaba 'evaluator_workflow', pero se obtuvo '{intention}'")
                return False
        
        print("‚úÖ Detecci√≥n de intenci√≥n funciona correctamente")
        
        # 5. Test directo del workflow
        print("\nüîß Testing evaluator workflow execution:")
        import asyncio
        
        # Crear contexto mock con datos requeridos
        import pandas as pd
        mock_context = {
            "dataframe": pd.DataFrame({"col1": [1, 2, 3], "col2": [4, 5, 6]}),
            "synthetic_data": pd.DataFrame({"col1": [1.1, 2.1, 3.1], "col2": [4.1, 5.1, 6.1]})
        }
        
        async def test_workflow():
            response = await orchestrator._execute_agent_workflow(
                "evaluator_workflow", 
                "evaluar calidad de datos", 
                mock_context
            )
            return response
        
        # Ejecutar el test
        response = asyncio.run(test_workflow())
        print(f"‚úÖ Workflow ejecutado. Respuesta tipo: {type(response)}")
        print(f"   Route: {response.get('route', 'N/A')}")
        print(f"   Agent: {response.get('agent', 'N/A')}")
        
        if response.get('route') == 'agent_workflow':
            print("‚úÖ Evaluator workflow funciona correctamente")
            return True
        else:
            print(f"‚ùå ERROR: Se esperaba route='agent_workflow', pero se obtuvo '{response.get('route')}'")
            print(f"   Mensaje: {response.get('message', 'N/A')[:100]}...")
            return False
            
    except Exception as e:
        print(f"‚ùå Error en test: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_evaluator_workflow()
    if success:
        print("\nüéâ TODOS LOS TESTS PASARON - Evaluator workflow est√° funcionando")
    else:
        print("\n‚ùå ALGUNOS TESTS FALLARON - Revisar configuraci√≥n")
